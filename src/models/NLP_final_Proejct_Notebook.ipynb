{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attention-based BiLSTM for Sentiment Analysis",
   "id": "cb0e826a2b69c718"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aspect based sentimental analysis with LSTM (Long Short-Term Memory) ",
   "id": "6726bcaa35238249"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T23:12:25.370017Z",
     "start_time": "2024-12-12T23:12:17.074475Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Import custom preprocessing functions\n",
    "# Ensure these modules are accessible in your project structure\n",
    "from src.preprocessing.preprocess_dataframe import preprocess_dataframe\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Polarity Encoding Mapping (including 'conflict')\n",
    "polarity_encoding = {\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "    'negative': 2,\n",
    "    'conflict': 3,  # Add 'conflict' to the mapping\n",
    "}\n",
    "\n",
    "train_csv_path = \"src/models/data/SemEval16/Train/Restaurants_Train.csv\"\n",
    "test_csv_path = \"src/models/data/SemEval16/Test/Restaurants_Test.csv\"\n",
    "\n",
    "restaurant_df_train = pd.read_csv(train_csv_path, encoding='utf8')\n",
    "test_df = pd.read_csv(test_csv_path, encoding='utf8')\n",
    "\n",
    "# Combine Train and Test Data\n",
    "df = pd.concat([restaurant_df_train, test_df], ignore_index=True)\n",
    "logger.info(f\"Combined dataset shape: {df.shape}\")\n",
    "\n",
    "# Apply Preprocessing\n",
    "new_df = preprocess_dataframe(df)\n",
    "logger.info(\"Dataframe after preprocessing:\")\n",
    "logger.info(new_df.head())\n",
    "\n",
    "# Function to Clean Text\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply Text Cleaning\n",
    "new_df['raw_text'] = new_df['raw_text'].apply(clean_text)\n",
    "new_df['aspect_term'] = new_df['aspect_term'].apply(clean_text)\n",
    "\n",
    "# Prepare Features and Labels\n",
    "X_raw_text = new_df['raw_text'].tolist()\n",
    "X_aspect = new_df['aspect_term'].tolist()\n",
    "y = new_df['polarity_encoded'].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split off the test set (15%)\n",
    "X_train_val_text, X_test_text, X_train_val_aspect, X_test_aspect, y_train_val, y_test = train_test_split(\n",
    "    X_raw_text, X_aspect, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Then, split the remaining 85% into 70% train and 15% validation\n",
    "# The validation size relative to the train_val set is 15/85 â‰ˆ 0.1765\n",
    "X_train_text, X_val_text, X_train_aspect, X_val_aspect, y_train, y_val = train_test_split(\n",
    "    X_train_val_text, X_train_val_aspect, y_train_val, test_size=0.1765, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "# Logging\n",
    "logger.info(f\"Training set size: {len(X_train_text)} ({len(X_train_text)/len(X_raw_text)*100:.2f}%)\")\n",
    "logger.info(f\"Validation set size: {len(X_val_text)} ({len(X_val_text)/len(X_raw_text)*100:.2f}%)\")\n",
    "logger.info(f\"Test set size: {len(X_test_text)} ({len(X_test_text)/len(X_raw_text)*100:.2f}%)\")\n",
    "\n",
    "# Vocabulary Builder Class\n",
    "class VocabularyBuilder:\n",
    "    def __init__(self, min_freq: int = 2):\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>'}\n",
    "        self.word_freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            self.word_freq.update(words)\n",
    "\n",
    "        idx = len(self.word2idx)\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq and word not in self.word2idx:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "        logger.info(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "\n",
    "    def text_to_indices(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text to list of indices\"\"\"\n",
    "        return [self.word2idx.get(word, self.word2idx['<unk>']) for word in text.split()]\n",
    "\n",
    "# Dataset Class\n",
    "class AspectSentimentDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], aspects: List[str], labels: np.ndarray, vocab: VocabularyBuilder):\n",
    "        self.texts = texts\n",
    "        self.aspects = aspects\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        text_indices = torch.LongTensor(self.vocab.text_to_indices(self.texts[idx]))\n",
    "        aspect_indices = torch.LongTensor(self.vocab.text_to_indices(self.aspects[idx]))\n",
    "        return text_indices, aspect_indices, self.labels[idx]\n",
    "\n",
    "# Collate Function for DataLoader\n",
    "def collate_fn(batch: List[Tuple]) -> Tuple:\n",
    "    \"\"\"Custom collate function to handle variable length sequences\"\"\"\n",
    "    texts, aspects, labels = zip(*batch)\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    aspects_padded = pad_sequence(aspects, batch_first=True, padding_value=0)\n",
    "    return texts_padded, aspects_padded, torch.stack(labels)\n",
    "\n",
    "# Attention Layer\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        # Adjusting dimensions: hidden_dim is the size of BiLSTM output (hidden_dim * 2)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs: torch.Tensor, mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.attention(encoder_outputs).squeeze(-1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        weighted_output = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        return weighted_output, attention_weights\n",
    "\n",
    "# Aspect Attention LSTM Model\n",
    "class AspectAttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int, num_layers: int = 2,\n",
    "                 dropout: float = 0.5):\n",
    "        super(AspectAttentionLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        self.sentence_lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.aspect_lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Attention layers for sentence and aspect\n",
    "        self.sentence_attention = AttentionLayer(hidden_dim)\n",
    "        self.aspect_attention = AttentionLayer(hidden_dim)\n",
    "\n",
    "        # Output layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Using concatenated attended outputs from both LSTMs\n",
    "        self.fc1 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def create_mask(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for padding tokens (0)\"\"\"\n",
    "        return (tensor != 0).float()\n",
    "\n",
    "    def forward(self, sentence: torch.Tensor, aspect: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Create masks for padding\n",
    "        sentence_mask = self.create_mask(sentence)\n",
    "        aspect_mask = self.create_mask(aspect)\n",
    "\n",
    "        # Embed inputs\n",
    "        sentence_embedded = self.embedding(sentence)  # (batch_size, seq_len, embed_dim)\n",
    "        aspect_embedded = self.embedding(aspect)  # (batch_size, aspect_len, embed_dim)\n",
    "\n",
    "        # Process through BiLSTM\n",
    "        sentence_outputs, _ = self.sentence_lstm(sentence_embedded)  # (batch_size, seq_len, hidden_dim*2)\n",
    "        aspect_outputs, _ = self.aspect_lstm(aspect_embedded)  # (batch_size, aspect_len, hidden_dim*2)\n",
    "\n",
    "        # Apply attention\n",
    "        sentence_context, sentence_weights = self.sentence_attention(sentence_outputs, sentence_mask)\n",
    "        aspect_context, aspect_weights = self.aspect_attention(aspect_outputs, aspect_mask)\n",
    "\n",
    "        # Concatenate the attended outputs\n",
    "        combined = torch.cat((sentence_context, aspect_context), dim=1)\n",
    "\n",
    "        # Final classification\n",
    "        output = self.dropout(combined)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output, sentence_weights, aspect_weights\n",
    "\n",
    "# Training Function\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                criterion: nn.Module, optimizer: optim.Optimizer, num_epochs: int,\n",
    "                device: torch.device) -> Dict:\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    best_val_acc = 0.0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_idx, (texts, aspects, labels) in enumerate(train_loader):\n",
    "            texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _, _ = model(texts, aspects)  # Ignore attention weights during training\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for texts, aspects, labels in val_loader:\n",
    "                texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "                outputs, _, _ = model(texts, aspects)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        logger.info(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        logger.info(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model_attention.pth')\n",
    "            logger.info(\"Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device: torch.device) -> Dict:\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_sentence_attention = []\n",
    "    all_aspect_attention = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, aspects, labels in test_loader:\n",
    "            texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "            outputs, sentence_attention, aspect_attention = model(texts, aspects)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_sentence_attention.extend(sentence_attention.cpu().numpy())\n",
    "            all_aspect_attention.extend(aspect_attention.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predictions, digits=4)\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    logger.info(f\"\\n{report}\")\n",
    "\n",
    "    return {\n",
    "        'predictions': all_predictions,\n",
    "        'true_labels': all_labels,\n",
    "        'sentence_attention': all_sentence_attention,\n",
    "        'aspect_attention': all_aspect_attention,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "# Check Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "logger.info(f'Using device: {device}')\n",
    "\n",
    "# Define Hyperparameters\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "MIN_WORD_FREQ = 2\n",
    "\n",
    "# Build Vocabulary\n",
    "vocab_builder = VocabularyBuilder(min_freq=MIN_WORD_FREQ)\n",
    "vocab_builder.build_vocab(X_train_text + X_train_aspect + X_val_text + X_val_aspect)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = AspectSentimentDataset(\n",
    "    texts=X_train_text,\n",
    "    aspects=X_train_aspect,\n",
    "    labels=y_train,\n",
    "    vocab=vocab_builder\n",
    ")\n",
    "val_dataset = AspectSentimentDataset(\n",
    "    texts=X_val_text,\n",
    "    aspects=X_val_aspect,\n",
    "    labels=y_val,\n",
    "    vocab=vocab_builder\n",
    ")\n",
    "test_dataset = AspectSentimentDataset(\n",
    "    texts=X_test_text,\n",
    "    aspects=X_test_aspect,\n",
    "    labels=y_test,\n",
    "    vocab=vocab_builder\n",
    ")\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "logger.info(f\"Number of training batches: {len(train_loader)}\")\n",
    "logger.info(f\"Number of validation batches: {len(val_loader)}\")\n",
    "logger.info(f\"Number of testing batches: {len(test_loader)}\")\n",
    "\n",
    "# Initialize model with attention\n",
    "model = AspectAttentionLSTM(\n",
    "    vocab_size=len(vocab_builder.word2idx),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=4,  # Including 'conflict'\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "logger.info(\"Model initialized.\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_attention.pth'))\n",
    "logger.info(\"Best model loaded for evaluation.\")\n",
    "\n",
    "# Evaluate the model on the Test Set\n",
    "evaluation_results = evaluate_model(model, test_loader, device)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samarthmahendra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:__main__:Combined dataset shape: (2676, 2)\n",
      "INFO:__main__:Dataframe after preprocessing:\n",
      "INFO:__main__:                                            raw_text aspect_term  \\\n",
      "0  Judging from previous posts this used to be a ...       place   \n",
      "1  We, there were four of us, arrived at noon - t...       staff   \n",
      "2  The food was lousy - too sweet or too salty an...        food   \n",
      "3  The food was lousy - too sweet or too salty an...    portions   \n",
      "4                                  Avoid this place!       place   \n",
      "\n",
      "   polarity_encoded  \n",
      "0                 2  \n",
      "1                 2  \n",
      "2                 2  \n",
      "3                 2  \n",
      "4                 2  \n",
      "INFO:__main__:Training set size: 1653 (69.95%)\n",
      "INFO:__main__:Validation set size: 355 (15.02%)\n",
      "INFO:__main__:Test set size: 355 (15.02%)\n",
      "INFO:__main__:Using device: mps\n",
      "INFO:__main__:Vocabulary size: 2188\n",
      "INFO:__main__:Number of training batches: 52\n",
      "INFO:__main__:Number of validation batches: 12\n",
      "INFO:__main__:Number of testing batches: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[62], line 405\u001B[0m\n\u001B[1;32m    395\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of testing batches: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_loader)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# Initialize model with attention\u001B[39;00m\n\u001B[1;32m    398\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAspectAttentionLSTM\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvocab_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword2idx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m    \u001B[49m\u001B[43membed_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEMBED_DIM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHIDDEN_DIM\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Including 'conflict'\u001B[39;49;00m\n\u001B[1;32m    403\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNUM_LAYERS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDROPOUT\u001B[49m\n\u001B[0;32m--> 405\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    406\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel initialized.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    408\u001B[0m \u001B[38;5;66;03m# Loss function and optimizer\u001B[39;00m\n",
      "File \u001B[0;32m~/bioinfo/NLPprojectv2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1337\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1338\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 1340\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/bioinfo/NLPprojectv2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    898\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 900\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    903\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    904\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    905\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    910\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    911\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/bioinfo/NLPprojectv2/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:283\u001B[0m, in \u001B[0;36mRNNBase._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn, recurse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weight_refs \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 283\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecurse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# Resets _flat_weights\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001B[39;00m\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001B[39;00m\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_flat_weights()\n",
      "File \u001B[0;32m~/bioinfo/NLPprojectv2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    923\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    925\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    926\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 927\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    928\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    930\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[0;32m~/bioinfo/NLPprojectv2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1320\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1321\u001B[0m             device,\n\u001B[1;32m   1322\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1323\u001B[0m             non_blocking,\n\u001B[1;32m   1324\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[1;32m   1325\u001B[0m         )\n\u001B[0;32m-> 1326\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1327\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1328\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1329\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1330\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1331\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1332\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dd791be2bf8b97b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aspect based sentimental analysis with Attention based LSTM (Long Short-Term Memory)",
   "id": "be60b4a7f2132d8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T23:12:56.700412Z",
     "start_time": "2024-12-12T23:12:30.607022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Dict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        # Adjusting dimensions: hidden_dim is the size of BiLSTM output (hidden_dim * 2)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "    \n",
    "    def forward(self, encoder_outputs: torch.Tensor, mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.attention(encoder_outputs).squeeze(-1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply attention weights to encoder outputs\n",
    "        weighted_output = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return weighted_output, attention_weights\n",
    "\n",
    "class AspectAttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int, num_layers: int = 2, dropout: float = 0.5):\n",
    "        super(AspectAttentionLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.sentence_lstm = nn.LSTM(\n",
    "            embed_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.aspect_lstm = nn.LSTM(\n",
    "            embed_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention layers for sentence and aspect\n",
    "        self.sentence_attention = AttentionLayer(hidden_dim)\n",
    "        self.aspect_attention = AttentionLayer(hidden_dim)\n",
    "        \n",
    "        # Output layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Using concatenated attended outputs from both LSTMs\n",
    "        self.fc1 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def create_mask(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for padding tokens (0)\"\"\"\n",
    "        return (tensor != 0).float()\n",
    "\n",
    "    def forward(self, sentence: torch.Tensor, aspect: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Create masks for padding\n",
    "        sentence_mask = self.create_mask(sentence)\n",
    "        aspect_mask = self.create_mask(aspect)\n",
    "        \n",
    "        # Embed inputs\n",
    "        sentence_embedded = self.embedding(sentence)  # (batch_size, seq_len, embed_dim)\n",
    "        aspect_embedded = self.embedding(aspect)      # (batch_size, aspect_len, embed_dim)\n",
    "        \n",
    "        # Process through BiLSTM\n",
    "        sentence_outputs, _ = self.sentence_lstm(sentence_embedded)  # (batch_size, seq_len, hidden_dim*2)\n",
    "        aspect_outputs, _ = self.aspect_lstm(aspect_embedded)        # (batch_size, aspect_len, hidden_dim*2)\n",
    "        \n",
    "        # Apply attention\n",
    "        sentence_context, sentence_weights = self.sentence_attention(sentence_outputs, sentence_mask)\n",
    "        aspect_context, aspect_weights = self.aspect_attention(aspect_outputs, aspect_mask)\n",
    "        \n",
    "        # Concatenate the attended outputs\n",
    "        combined = torch.cat((sentence_context, aspect_context), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.dropout(combined)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        \n",
    "        return output, sentence_weights, aspect_weights\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                criterion: nn.Module, optimizer: optim.Optimizer, num_epochs: int, \n",
    "                device: torch.device) -> Dict:\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    best_val_acc = 0.0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch_idx, (texts, aspects, labels) in enumerate(train_loader):\n",
    "            texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _, _ = model(texts, aspects)  # Ignore attention weights during training\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for texts, aspects, labels in val_loader:\n",
    "                texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "                outputs, _, _ = model(texts, aspects)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model_attention.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, device: torch.device) -> Dict:\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_sentence_attention = []\n",
    "    all_aspect_attention = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, aspects, labels in test_loader:\n",
    "            texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "            outputs, sentence_attention, aspect_attention = model(texts, aspects)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_sentence_attention.extend(sentence_attention.cpu().numpy())\n",
    "            all_aspect_attention.extend(aspect_attention.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predictions, digits=4)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return {\n",
    "        'predictions': all_predictions,\n",
    "        'true_labels': all_labels,\n",
    "        'sentence_attention': all_sentence_attention,\n",
    "        'aspect_attention': all_aspect_attention,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "# Initialize model with attention\n",
    "model = AspectAttentionLSTM(\n",
    "    vocab_size=len(vocab_builder.word2idx),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=4,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation_results = evaluate_model(model, test_loader, device)"
   ],
   "id": "37a8eafb7ecc5bf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "Train Loss: 0.7446, Val Loss: 0.5567, Val Accuracy: 73.80%\n",
      "Epoch 2/20:\n",
      "Train Loss: 0.4577, Val Loss: 0.5049, Val Accuracy: 77.75%\n",
      "Epoch 3/20:\n",
      "Train Loss: 0.2738, Val Loss: 0.5349, Val Accuracy: 79.72%\n",
      "Epoch 4/20:\n",
      "Train Loss: 0.1803, Val Loss: 0.6521, Val Accuracy: 75.49%\n",
      "Epoch 5/20:\n",
      "Train Loss: 0.1367, Val Loss: 0.7964, Val Accuracy: 72.96%\n",
      "Epoch 6/20:\n",
      "Train Loss: 0.1303, Val Loss: 0.9900, Val Accuracy: 78.59%\n",
      "Early stopping triggered\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.0667    0.1250        15\n",
      "           1     0.8039    0.9572    0.8739       257\n",
      "           2     0.6667    0.3855    0.4885        83\n",
      "\n",
      "    accuracy                         0.7859       355\n",
      "   macro avg     0.8235    0.4698    0.4958       355\n",
      "weighted avg     0.7801    0.7859    0.7522       355\n",
      "\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aspect based sentimental analysis with ATAE-LSTM (Aspect Term Attention based LSTM)",
   "id": "8bcb8da24f34a05e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-12T23:13:11.705269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Import the preprocessing function\n",
    "from src.preprocessing.preprocess_dataframe import preprocess_dataframe\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define hyperparameters\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 100\n",
    "NUM_CLASSES = 4  # neutral, positive, negative, conflict\n",
    "\n",
    "# Define polarity encoding\n",
    "polarity_encoding = {\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "    'negative': 2,\n",
    "    'conflict': 3,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "train_path = \"src/models/data/SemEval16/Train/Restaurants_Train.csv\"\n",
    "test_path = \"src/models/data/SemEval16/Test/Restaurants_Test.csv\"\n",
    "\n",
    "# Load the training and testing data\n",
    "restaurant_df_train = pd.read_csv(train_path, encoding='utf8')\n",
    "test_df_original = pd.read_csv(test_path, encoding='utf8')\n",
    "\n",
    "# Combine train and test data for preprocessing\n",
    "df = pd.concat([restaurant_df_train, test_df_original], ignore_index=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "new_df = preprocess_dataframe(df)\n",
    "\n",
    "# Split into train (70%) and temp (30%) sets with stratification\n",
    "train_df, temp_df = train_test_split(\n",
    "    new_df,\n",
    "    test_size=0.30,\n",
    "    random_state=SEED,\n",
    "    stratify=new_df['polarity_encoded']\n",
    ")\n",
    "\n",
    "# Further split temp into validation (15%) and test (15%) sets\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.50,  # 50% of 30% is 15%\n",
    "    random_state=SEED,\n",
    "    stratify=temp_df['polarity_encoded']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Testing samples: {len(test_df)}\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "class AspectDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        text = row['raw_text']\n",
    "        aspect = row['aspect_term']\n",
    "        label = row['polarity_encoded']\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Tokenize the aspect\n",
    "        aspect_encoding = self.tokenizer.encode_plus(\n",
    "            aspect,\n",
    "            add_special_tokens=True,\n",
    "            max_length=10,  # assuming aspect terms are short\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Squeeze to remove extra dimensions\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        aspect_ids = aspect_encoding['input_ids'].squeeze()\n",
    "\n",
    "        return input_ids, aspect_ids, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AspectDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_dataset = AspectDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_dataset = AspectDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "class ATAEAttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int):\n",
    "        super(ATAEAttentionLayer, self).__init__()\n",
    "        # Accommodate concatenated hidden states and aspect embeddings\n",
    "        self.attention = nn.Linear(hidden_dim * 4, 1)  # hidden_dim * 2 (BiLSTM) * 2 (hidden + aspect)\n",
    "\n",
    "    def forward(self,\n",
    "                encoder_outputs: torch.Tensor,\n",
    "                aspect_embedding: torch.Tensor,\n",
    "                mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # encoder_outputs: (batch_size, seq_len, hidden_dim * 2)\n",
    "        # aspect_embedding: (batch_size, hidden_dim * 2)\n",
    "\n",
    "        batch_size, seq_len, hidden_dim = encoder_outputs.size()\n",
    "\n",
    "        # Repeat aspect_embedding across the sequence length\n",
    "        aspect_repeated = aspect_embedding.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # Concatenate encoder outputs with aspect embeddings\n",
    "        combined = torch.cat([encoder_outputs, aspect_repeated], dim=2)  # (batch_size, seq_len, hidden_dim * 4)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attention_scores = self.attention(combined).squeeze(-1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_len)\n",
    "\n",
    "        # Apply attention weights to encoder outputs\n",
    "        weighted_output = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(\n",
    "            1)  # (batch_size, hidden_dim * 2)\n",
    "\n",
    "        return weighted_output, attention_weights\n",
    "\n",
    "\n",
    "class ATAELSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 embed_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 num_classes: int,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout: float = 0.5,\n",
    "                 pretrained_embeddings: torch.Tensor = None):\n",
    "        super(ATAELSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Embedding layer for both words and aspects\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        # Main LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim + embed_dim,  # word embedding + aspect embedding\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Aspect LSTM\n",
    "        self.aspect_lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # ATAE attention layer\n",
    "        self.attention = ATAEAttentionLayer(hidden_dim)\n",
    "\n",
    "        # Output layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def create_mask(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create mask for padding tokens (0)\"\"\"\n",
    "        return (tensor != 0).float()\n",
    "\n",
    "    def forward(self, sentence: torch.Tensor, aspect: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Create masks\n",
    "        sentence_mask = self.create_mask(sentence)\n",
    "\n",
    "        # Embed inputs\n",
    "        sentence_embedded = self.embedding(sentence)  # (batch_size, seq_len, embed_dim)\n",
    "        aspect_embedded = self.embedding(aspect)  # (batch_size, aspect_len, embed_dim)\n",
    "\n",
    "        # Get aspect representation\n",
    "        aspect_output, (aspect_hidden, _) = self.aspect_lstm(aspect_embedded)\n",
    "        # Concatenate forward and backward hidden states\n",
    "        aspect_repr = torch.cat([aspect_hidden[-2], aspect_hidden[-1]], dim=1)  # (batch_size, hidden_dim * 2)\n",
    "\n",
    "        # Repeat aspect embedding for each word in the sentence\n",
    "        batch_size, seq_len, _ = sentence_embedded.size()\n",
    "        aspect_repeated = aspect_embedded.mean(1).unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate word embeddings with repeated aspect embeddings\n",
    "        combined_input = torch.cat([sentence_embedded, aspect_repeated], dim=2)\n",
    "\n",
    "        # Process through LSTM\n",
    "        lstm_outputs, _ = self.lstm(combined_input)\n",
    "\n",
    "        # Apply attention\n",
    "        attended_output, attention_weights = self.attention(\n",
    "            lstm_outputs,\n",
    "            aspect_repr,\n",
    "            sentence_mask\n",
    "        )\n",
    "\n",
    "        # Final classification\n",
    "        output = self.dropout(attended_output)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def train_atae_model(model: nn.Module,\n",
    "                     train_loader: DataLoader,\n",
    "                     val_loader: DataLoader,\n",
    "                     criterion: nn.Module,\n",
    "                     optimizer: optim.Optimizer,\n",
    "                     num_epochs: int,\n",
    "                     device: torch.device) -> Dict:\n",
    "    \"\"\"\n",
    "    Train the ATAE-LSTM model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The ATAE-LSTM model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (optim.Optimizer): Optimizer.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        device (torch.device): Device to train on.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Training history containing losses and accuracies.\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    best_val_acc = 0.0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_idx, (texts, aspects, labels) in enumerate(train_loader):\n",
    "            texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(texts, aspects)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for texts, aspects, labels in val_loader:\n",
    "                texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "                outputs, _ = model(texts, aspects)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early Stopping and Checkpointing\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model_atae.pth')\n",
    "            print(\"Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_atae_model(model: nn.Module, data_loader: DataLoader, device: torch.device) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate the ATAE-LSTM model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained ATAE-LSTM model.\n",
    "        data_loader (DataLoader): DataLoader for evaluation data.\n",
    "        device (torch.device): Device to evaluate on.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Evaluation results including predictions, true labels, attention weights, and classification report.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_attention_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for texts, aspects, labels in data_loader:\n",
    "            texts, aspects, labels = texts.to(device), aspects.to(device), labels.to(device)\n",
    "            outputs, attention_weights = model(texts, aspects)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_attention_weights.extend(attention_weights.cpu().numpy())\n",
    "\n",
    "    report = classification_report(all_labels, all_predictions, digits=4)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return {\n",
    "        'predictions': all_predictions,\n",
    "        'true_labels': all_labels,\n",
    "        'attention_weights': all_attention_weights,\n",
    "        'classification_report': report\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# Use tokenizer's vocab size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = ATAELSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pretrained_embeddings=None  # Replace with pretrained_embeddings if available\n",
    ").to(device)\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "y_train = train_df['polarity_encoded'].tolist()\n",
    "class_counts = Counter(y_train)\n",
    "print(\"Class Counts:\", class_counts)\n",
    "\n",
    "total_samples = len(y_train)\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "weights = torch.tensor([class_weights.get(i, 1.0) for i in range(NUM_CLASSES)], dtype=torch.float32).to(device)\n",
    "print(\"Class Weights:\", weights)\n",
    "\n",
    "# Define loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Define optimizer with weight decay for L2 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "# Train the model\n",
    "history = train_atae_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,  # Use validation set\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model_atae.pth'))\n",
    "print(\"Best model loaded for evaluation.\")\n",
    "\n",
    "# Evaluate on Validation Set\n",
    "print(\"\\nEvaluating on Validation Set:\")\n",
    "evaluation_val = evaluate_atae_model(model, val_loader, device)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "print(\"\\nEvaluating on Test Set:\")\n",
    "evaluation_test = evaluate_atae_model(model, test_loader, device)\n",
    "\n",
    "# (Optional) Plotting Training and Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# (Optional) Plotting Validation Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "6702f76ad168f05a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training samples: 1654\n",
      "Validation samples: 354\n",
      "Testing samples: 355\n",
      "Number of training batches: 52\n",
      "Number of validation batches: 12\n",
      "Number of testing batches: 12\n",
      "Class Counts: Counter({1: 1196, 2: 389, 0: 69})\n",
      "Class Weights: tensor([23.9710,  1.3829,  4.2519,  1.0000], device='mps:0')\n",
      "Epoch 1/20:\n",
      "Train Loss: 1.1872, Val Loss: 1.0136, Val Accuracy: 69.77%\n",
      "Best model saved.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Aspect based sentimental analysis with BERT variant of ATAE-LSTM",
   "id": "c3863bb397130bc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T21:01:57.489572Z",
     "start_time": "2024-12-12T20:50:19.356533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Import custom preprocessing functions\n",
    "# Ensure that these modules are available in your project\n",
    "from src.preprocessing.preprocess_dataframe import preprocess_dataframe\n",
    "from src.preprocessing.vocabulary_builder import VocabularyBuilder\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Data Preparation\n",
    "# -----------------------------\n",
    "\n",
    "# Polarity encoding mapping (including 'conflict')\n",
    "polarity_encoding = {\n",
    "    'neutral': 0,\n",
    "    'positive': 1,\n",
    "    'negative': 2,\n",
    "    'conflict': 3,  # Added 'conflict' to the mapping\n",
    "}\n",
    "\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(train_path, test_path):\n",
    "    # Load the training and testing data\n",
    "    restaurant_df_train = pd.read_csv(\n",
    "        train_path,\n",
    "        encoding='utf8'\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        test_path,\n",
    "        encoding='utf8'\n",
    "    )\n",
    "\n",
    "    # Combine both train and test data for preprocessing\n",
    "    df = pd.concat([restaurant_df_train, test_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    return preprocess_dataframe(df)\n",
    "\n",
    "\n",
    "\n",
    "# Paths to the dataset\n",
    "train_csv_path = \"src/models/data/SemEval16/Train/Restaurants_Train.csv\"\n",
    "test_csv_path = \"src/models/data/SemEval16/Test/Restaurants_Test.csv\"\n",
    "\n",
    "# Load and preprocess data\n",
    "df = load_and_preprocess_data(train_csv_path, test_csv_path)\n",
    "\n",
    "# Split into train, validation, and test sets with stratification\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df['polarity_encoded']\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_val_df['polarity_encoded']\n",
    ")\n",
    "\n",
    "# Reset index after split\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Device Configuration\n",
    "# -----------------------------\n",
    "\n",
    "# Check for available device (preferably GPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  # For Apple Silicon Macs\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Tokenizer and BERT Model Setup\n",
    "# -----------------------------\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Dataset and DataLoader\n",
    "# -----------------------------\n",
    "\n",
    "class BertAspectDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for BERT-based Aspect Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        sentence = row['raw_text']\n",
    "        aspect = row['aspect_term']\n",
    "        label = row['polarity_encoded']\n",
    "\n",
    "        # Use tokenizer's ability to handle two separate texts\n",
    "        encoding = self.tokenizer(\n",
    "            text=sentence,\n",
    "            text_pair=aspect,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),  # [max_length]\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),  # [max_length]\n",
    "            'token_type_ids': encoding['token_type_ids'].squeeze(),  # [max_length]\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = BertAspectDataset(train_df, tokenizer, max_length=128)\n",
    "val_dataset = BertAspectDataset(val_df, tokenizer, max_length=128)\n",
    "test_dataset = BertAspectDataset(test_df, tokenizer, max_length=128)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Model Definition\n",
    "# -----------------------------\n",
    "\n",
    "class BERT_AttentionLSTM_Enhanced(nn.Module):\n",
    "    \"\"\"\n",
    "    Integrated BERT with LSTM and Enhanced Attention for Aspect-Based Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert_model, hidden_dim, num_classes):\n",
    "        super(BERT_AttentionLSTM_Enhanced, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.sentence_lstm = nn.LSTM(\n",
    "            bert_model.config.hidden_size,\n",
    "            hidden_dim,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.aspect_lstm = nn.LSTM(\n",
    "            bert_model.config.hidden_size,\n",
    "            hidden_dim,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attention = nn.Linear(hidden_dim * 4, 1)  # Enhanced attention\n",
    "        self.fc = nn.Linear(hidden_dim * 4, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Pass combined sentence and aspect through BERT\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = bert_outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Split the sequence into sentence and aspect parts based on token_type_ids\n",
    "        # token_type_ids == 0: sentence, ==1: aspect\n",
    "        sentence_mask = token_type_ids == 0  # [batch, seq_len]\n",
    "        aspect_mask = token_type_ids == 1  # [batch, seq_len]\n",
    "\n",
    "        # Extract sentence embeddings\n",
    "        sentence_embs = last_hidden_state * sentence_mask.unsqueeze(-1)  # [batch, seq_len, hidden_size]\n",
    "        # Extract aspect embeddings\n",
    "        aspect_embs = last_hidden_state * aspect_mask.unsqueeze(-1)  # [batch, seq_len, hidden_size]\n",
    "\n",
    "        # Pass through LSTMs\n",
    "        sentence_lstm_out, _ = self.sentence_lstm(sentence_embs)  # [batch, seq_len, hidden_dim*2]\n",
    "        aspect_lstm_out, _ = self.aspect_lstm(aspect_embs)  # [batch, seq_len, hidden_dim*2]\n",
    "\n",
    "        # Compute mean of aspect embeddings\n",
    "        aspect_mean = torch.mean(aspect_lstm_out, dim=1).unsqueeze(1)  # [batch, 1, hidden_dim*2]\n",
    "\n",
    "        # Tile aspect_mean to match sentence length\n",
    "        aspect_tiled = aspect_mean.repeat(1, sentence_lstm_out.size(1), 1)  # [batch, sentence_len, hidden_dim*2]\n",
    "\n",
    "        # Combine sentence and aspect embeddings\n",
    "        combined_embs = torch.cat([sentence_lstm_out, aspect_tiled], dim=-1)  # [batch, sentence_len, hidden_dim*4]\n",
    "\n",
    "        # Apply enhanced attention\n",
    "        attention_scores = self.attention(combined_embs).squeeze(-1)  # [batch, sentence_len]\n",
    "        alpha = torch.softmax(attention_scores, dim=1).unsqueeze(-1)  # [batch, sentence_len, 1]\n",
    "        attended = combined_embs * alpha  # [batch, sentence_len, hidden_dim*4]\n",
    "        context = torch.sum(attended, dim=1)  # [batch, hidden_dim*4]\n",
    "\n",
    "        # Pass through fully connected layer\n",
    "        out = self.dropout(context)\n",
    "        logits = self.fc(out)  # [batch, num_classes]\n",
    "\n",
    "        return logits, alpha\n",
    "\n",
    "\n",
    "# Initialize the enhanced model\n",
    "hidden_dim = 256\n",
    "num_classes = len(polarity_encoding)  # neutral, positive, negative, conflict\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = BERT_AttentionLSTM_Enhanced(bert_model, hidden_dim, num_classes).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Training Setup\n",
    "# -----------------------------\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "y_train = train_df['polarity_encoded'].tolist()\n",
    "class_counts = Counter(y_train)\n",
    "print(\"Class Counts:\", class_counts)  # Debugging line\n",
    "\n",
    "total_samples = len(y_train)\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "weights = torch.tensor([class_weights.get(i, 1.0) for i in range(num_classes)], dtype=torch.float32).to(device)\n",
    "print(\"Class Weights:\", weights)  # Debugging line\n",
    "\n",
    "# Define loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Define optimizer with a higher learning rate\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Define scheduler\n",
    "EPOCHS = 10\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Training and Evaluation Functions\n",
    "# -----------------------------\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs, _ = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_train\n",
    "    epoch_acc = correct_train / total_train\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a validation or test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs, _ = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    epoch_f1 = f1_score(labels_list, preds_list, average='weighted')\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1, preds_list, labels_list\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Training Loop with Early Stopping\n",
    "# -----------------------------\n",
    "\n",
    "# Early Stopping parameters\n",
    "best_f1 = 0\n",
    "patience = 3\n",
    "counter = 0\n",
    "best_model_path = 'best_model.pt'\n",
    "\n",
    "# Lists to store metrics\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "val_loss_list = []\n",
    "val_f1_list = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(model, val_loader, criterion, device)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_f1_list.append(val_f1)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}, Validation F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"Best model saved.\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement in F1 for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Inference on Sample Sentence\n",
    "# -----------------------------\n",
    "\n",
    "test_sentence = (\n",
    "    \"The decor is night tho...but they REALLY need to clean that vent in the ceiling...its quite \"\n",
    "    \"un-appetizing, and kills your effort to make this place look sleek and modern.\"\n",
    ")\n",
    "test_aspects = [\"place\", \"decor\", \"vent\"]\n",
    "\n",
    "\n",
    "def preprocess_input_bert_embeddings(sentence, aspect, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess the input sentence and aspect to obtain BERT inputs.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The raw sentence.\n",
    "        aspect (str): The aspect term.\n",
    "        tokenizer: BERT tokenizer.\n",
    "        max_length (int): Maximum token length.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing 'input_ids', 'attention_mask', and 'token_type_ids'.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text=sentence,\n",
    "        text_pair=aspect,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': encoding['input_ids'].squeeze().to(device),\n",
    "        'attention_mask': encoding['attention_mask'].squeeze().to(device),\n",
    "        'token_type_ids': encoding['token_type_ids'].squeeze().to(device)\n",
    "    }\n",
    "\n",
    "\n",
    "# Invert polarity encoding for interpretation\n",
    "inv_polarity = {v: k for k, v in polarity_encoding.items()}\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "print(\"\\nInference on Sample Sentence:\")\n",
    "with torch.no_grad():\n",
    "    for aspect in test_aspects:\n",
    "        # Preprocess the input sentence and aspect\n",
    "        inputs = preprocess_input_bert_embeddings(test_sentence, aspect, tokenizer, max_length=128)\n",
    "        input_ids = inputs['input_ids'].unsqueeze(0)  # [1, max_length]\n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(0)  # [1, max_length]\n",
    "        token_type_ids = inputs['token_type_ids'].unsqueeze(0)  # [1, max_length]\n",
    "\n",
    "        # Predict using the trained model\n",
    "        outputs, attention_weights = model(input_ids, attention_mask, token_type_ids)\n",
    "        pred_label = torch.argmax(outputs, dim=1).item()\n",
    "\n",
    "        print(f\"Aspect: '{aspect}', Predicted Sentiment: {inv_polarity[pred_label]}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Visualization & Metrics\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Class Distribution Plot\n",
    "def plot_class_distribution(train_df, val_df, test_df, polarity_encoding):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    sns.countplot(x='polarity_encoded', data=train_df, ax=ax[0], palette='viridis')\n",
    "    ax[0].set_title('Training Set Class Distribution')\n",
    "    ax[0].set_xlabel('Polarity')\n",
    "    ax[0].set_ylabel('Count')\n",
    "    ax[0].set_xticklabels([k for k, v in sorted(polarity_encoding.items(), key=lambda item: item[1])])\n",
    "\n",
    "    sns.countplot(x='polarity_encoded', data=val_df, ax=ax[1], palette='viridis')\n",
    "    ax[1].set_title('Validation Set Class Distribution')\n",
    "    ax[1].set_xlabel('Polarity')\n",
    "    ax[1].set_ylabel('Count')\n",
    "    ax[1].set_xticklabels([k for k, v in sorted(polarity_encoding.items(), key=lambda item: item[1])])\n",
    "\n",
    "    sns.countplot(x='polarity_encoded', data=test_df, ax=ax[2], palette='viridis')\n",
    "    ax[2].set_title('Testing Set Class Distribution')\n",
    "    ax[2].set_xlabel('Polarity')\n",
    "    ax[2].set_ylabel('Count')\n",
    "    ax[2].set_xticklabels([k for k, v in sorted(polarity_encoding.items(), key=lambda item: item[1])])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_class_distribution(train_df, val_df, test_df, polarity_encoding)\n",
    "\n",
    "\n",
    "# 2. Confusion Matrix & Classification Report on Test Set\n",
    "def evaluate_on_test_set(model, test_loader, criterion, device, polarity_encoding):\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs, _ = model(input_ids, attention_mask, token_type_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(labels_list, preds_list)\n",
    "\n",
    "    # Define labels in order\n",
    "    labels_order = sorted(polarity_encoding, key=polarity_encoding.get)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[\"neutral\", 'positive', \"negative\"],\n",
    "                yticklabels=[\"neutral\", 'positive', \"negative\"])\n",
    "    plt.title('Confusion Matrix on Test Set')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    # Generate and print classification report\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    print(classification_report(labels_list, preds_list, target_names=[\"neutral\", 'positive', \"negative\"]))\n",
    "\n",
    "    # Overall Accuracy\n",
    "    overall_acc = accuracy_score(labels_list, preds_list)\n",
    "    print(f\"Overall Test Accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "\n",
    "evaluate_on_test_set(model, test_loader, criterion, device, polarity_encoding)\n",
    "\n",
    "\n",
    "# 3. Training and Validation Metrics Plot\n",
    "def plot_training_history(train_loss, train_acc, val_loss, val_acc, val_f1):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy and F1\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "    plt.plot(epochs, val_f1, 'g-', label='Validation F1 Score')\n",
    "    plt.title('Training and Validation Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(train_loss_list, train_acc_list, val_loss_list, val_acc_list, val_f1_list)\n",
    "\n"
   ],
   "id": "d555533a8ad060cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Class Counts: Counter({1: 1366, 2: 445, 0: 79})\n",
      "Class Weights: tensor([23.9241,  1.3836,  4.2472,  1.0000], device='mps:0')\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:05<00:00,  1.82it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1445, Train Acc: 0.5593\n",
      "Test Loss: 0.8677, Test Acc: 0.7844, Test F1: 0.7799\n",
      "Best model saved.\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:03<00:00,  1.88it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6961, Train Acc: 0.8429\n",
      "Test Loss: 0.7392, Test Acc: 0.8182, Test F1: 0.8312\n",
      "Best model saved.\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:02<00:00,  1.89it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:05<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4439, Train Acc: 0.9180\n",
      "Test Loss: 0.9671, Test Acc: 0.8562, Test F1: 0.8594\n",
      "Best model saved.\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:07<00:00,  1.77it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2904, Train Acc: 0.9545\n",
      "Test Loss: 1.2125, Test Acc: 0.8901, Test F1: 0.8822\n",
      "Best model saved.\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:01<00:00,  1.93it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2449, Train Acc: 0.9725\n",
      "Test Loss: 1.2807, Test Acc: 0.8879, Test F1: 0.8824\n",
      "Best model saved.\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:00<00:00,  1.97it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0912, Train Acc: 0.9836\n",
      "Test Loss: 1.3833, Test Acc: 0.8943, Test F1: 0.8895\n",
      "Best model saved.\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:03<00:00,  1.86it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411, Train Acc: 0.9942\n",
      "Test Loss: 1.4662, Test Acc: 0.9091, Test F1: 0.9028\n",
      "Best model saved.\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:06<00:00,  1.80it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0230, Train Acc: 0.9942\n",
      "Test Loss: 1.5474, Test Acc: 0.9006, Test F1: 0.8958\n",
      "No improvement in F1 for 1 epoch(s).\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:04<00:00,  1.85it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0123, Train Acc: 0.9968\n",
      "Test Loss: 1.6315, Test Acc: 0.9027, Test F1: 0.8968\n",
      "No improvement in F1 for 2 epoch(s).\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [01:04<00:00,  1.86it/s]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0096, Train Acc: 0.9979\n",
      "Test Loss: 1.6379, Test Acc: 0.9049, Test F1: 0.8988\n",
      "No improvement in F1 for 3 epoch(s).\n",
      "Early stopping triggered.\n",
      "\n",
      "Inference on Sample Sentence:\n",
      "Aspect: 'place', Predicted Sentiment: negative\n",
      "Aspect: 'decor', Predicted Sentiment: positive\n",
      "Aspect: 'vent', Predicted Sentiment: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "INFO:matplotlib.category:Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHqCAYAAADVi/1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/GU6VOAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYxUlEQVR4nO3deVxU9f7H8fcAsigSKrhimkvuIkJoi7mkuXHL0Moll9TUK8StXAptMZe4ieYCuOBumpqiLWZZWdcy1zBQ82eBVpIrqGgmgsL8/vA63UlFUDkzMK/n4zGPmu/3nJnPoXNnPvd9ljGZzWazAAAAAAAAAAM52boAAAAAAAAAOB5CKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilABR7ZrPZ1iUUW/bwt7OHGgAAkEred1JJ2x4j2cPfzh5qAIoaoRRgkFdeeUX16tXL99G3b9/beo+YmBjVq1evyNe5HWfOnFFUVJTat2+vxo0bKzg4WP3799cXX3xR6NfKycnRW2+9pY8//rhAy2/btk3h4eFq1aqV/P391bFjR7399ts6deqUZZnff/9d9erV09q1awtdz53Wrl07q/2jQYMGCgoKUq9evfTBBx9cs3y9evUUExNT4NdfvXq13n777Zsu17dvX6t9s7DvcyPHjx/XkCFDdOTIEctYu3bt9Morr9z2awMASgYj+qerZs2apQULFlie0yPRI90MPRJw+1xsXQDgKIYPH66ePXtans+aNUv79+9XbGysZczT0/O23uPJJ59Uq1atinydW3Xx4kX16dNHubm5GjJkiGrUqKE//vhDn376qcLDwzVmzBj179+/wK938uRJLVmyRFFRUTdddsqUKZo/f746deqksWPHytvbWz/99JPmzZunzz//XMuWLVOVKlVuZ/OKROvWrTV8+HBJ0uXLl3XmzBl9+umnevnll/V///d/ioyMtCy7atUqVa5cucCvPXv2bAUHB990uTfeeKPwhRfA1q1btXnzZqux2NjY2/7fAQCg5DCif7pqxowZCg8PtzynR6JHuhl6JOD2EUoBBrn77rt19913W56XL19erq6uatas2R17j8qVKxfqC/dW17lVn332mQ4ePKiNGzeqZs2alvH27dvr4sWLmjlzpp555hk5Ozvf0ff95JNPNG/ePEVGRmrAgAGW8ZYtW6p169Z64oknNGnSJKsG116UL1/+mn2kQ4cO8vX11eLFi/Xoo48qMDBQku7ovvS/6tSpUySvez0NGzY07L0AAPbPiP7pRuiR6JFuhh4JuH1cvgfYmbVr16phw4ZavXq1HnzwQQUHBys1NVW5ubmKj49XSEiImjZtqmbNmqlnz57avn27Zd2/n2bet29fjR07VvHx8WrTpo2aNGminj17as+ePbe1jiT95z//UWhoqJo2baqOHTtq/fr16tChQ76nLGdkZEiS8vLyrpkbOnSohg8frpycHMvYzz//rKFDh6p58+Zq3ry5wsLClJaWJunKKeSPPPKIJCkyMlLt2rW74fvGx8erTp061z3CWLNmTY0aNUoBAQE3vG5/165dGjRokO677z41btxY7dq1U0xMjNV2rF+/Xo899piaNm2qli1bauTIkTpx4oRlft++ferfv78CAwMVEBCgAQMGKCkp6YY130x4eLjc3Ny0cuVKy9jfTxlfsmSJOnXqpCZNmqhVq1YaN26czp8/L+nKKeBHjhzRunXrVK9ePf3+++833Pf+fmq6JJ0/f14jR45UQECA7r//fk2cOFFZWVmW+eudYr527Vqr97p6BPORRx6xLPv39f744w/LpQxNmjRRSEiI1qxZY/W67dq108yZM/X222/rgQceUNOmTTVo0CD9+uuvt/z3BQAUL99//72eeeYZ+fv7Kzg4WC+//LJOnz5tmc/Ly9O0adPUrl07y3f51KlTdenSJUmy9EKxsbGWf6dHokeiRwKKHqEUYIdyc3O1cOFCTZo0SZGRkapdu7amTJmiWbNm6emnn9b8+fM1YcIEZWZm6l//+pfVF93fbdy4UZs2bdKrr76qd955RxkZGXr++eeVm5t7y+ts375dw4cPV5UqVRQTE6M+ffrojTfe0LFjx/LdrlatWsnFxUX9+/dXbGyskpKSLM3g1S9JDw8PSdIvv/yinj176tSpU3r77bc1adIkpaWlqVevXjp16pQqVqxoOWr3z3/+84ZH8NLT03XgwAG1adNGJpPpusv07t1bgwYNuu78gQMHNGDAAHl7e2vatGmaPXu2goKCFBsbq08//VSSlJiYqNGjR+vRRx+1HG3cvn27RowYIelKczJ48GCVK1dOMTExmjZtmrKysjRo0CD98ccf+f7NbqRs2bJq2rSpEhMTrzu/fv16RUdHq0+fPlqwYIHCwsL04YcfasKECZKuNN2+vr5q3bq1Vq1apYoVK0q6/r53Pe+++67+/PNPTZ8+XUOHDtXq1as1cuTIAtffpk0b/fOf/7TUcvX0+/918eJF9e7dWx9//LEGDx6sWbNmKTAwUGPHjtWcOXOsll26dKkOHTqkqKgoTZw4Ufv27dPLL79c4HoAAMXXrl27NGDAALm7u2v69OkaM2aMdu7cqX79+unixYuSpHnz5mnFihUKCwvTwoUL1atXLy1YsECzZ8+WdOXyLknq0aOH5d+vhx7pL/RI9EjAncDle4CdGjZsmNq0aWN5fvLkSb344otWR2Pc3Nz0/PPP66effrrhacmXL1/WggULLNeg//nnn5Zr7Rs3bnxL68TExKhu3bqKjY21NCkVKlTQSy+9lO821atXT9OmTdObb76pmJgYxcTEyN3dXUFBQerRo4c6d+5sWTY2NlYeHh5avHixpY77779f7du31/z58/Xyyy+rQYMGkq6c2n+jU5qvNoF+fn751nYjBw4c0AMPPKDo6Gg5OV3J8R988EF99dVX2rFjh7p27arExES5u7tryJAhcnV1lSR5e3tr7969MpvNSk1N1ZkzZ9SvXz81b95cklSrVi2tWrVKf/75p8qWLXtLtfn4+FxzdPaqnTt3ys/PT3369JGTk5OCg4NVunRpnT17VtKVU8BdXV2ve+r73/e966ldu7bi4uLk5OSk1q1by2Qy6a233tLPP/+se++996a1ly9f3nI5RoMGDa7732ft2rX6+eeftXLlSgUEBEi60rRfvnxZs2bNUs+ePeXt7S1J8vLy0qxZsyyXNRw+fFgxMTE6c+aMypUrd9N6AADF19SpU3XPPfdo7ty5lu8Bf39/de3aVQkJCerTp4927typxo0bq3v37pKk4OBgeXh4WL6Dr34XVq5cOd9LveiR/kKPdH30SEDhcKYUYKeuNhNXTZ06Vf3799fp06f1/fffKyEhQR999JEkWZ3O/Xd16tSxuilipUqVJCnfs6vyWycnJ0c//PCDHn30UaujZp06dZKLy81z7kcffVT/+c9/NH/+fA0cOFC1a9fW1q1b9cILLygiIsJyevj27dsVHBwsd3d3Xb58WZcvX5anp6eCgoK0devWm77PVVdrut7p8AXRrVs3zZs3T5cuXdKBAwe0ceNGzZw5U7m5uZYjmPfdd5+ysrIUEhKiqVOn6vvvv9dDDz2k8PBwmUwm1a1bV+XLl9ewYcP0+uuv64svvpCPj49GjRp1W/eqMJvNNzyy2bJlS/3yyy8KDQ1VbGys9u7dq3/84x8F+oWiv+9719OpUydLAypd+e8qXTlafafs3LlT1apVszRbVz322GPKzs5WcnKyZaxJkyZW99m4+nfNbz8HABR/WVlZSk5OVuvWrWU2my09Q/Xq1VW7dm199913kqQWLVrou+++U+/evTV//nylpqbqmWee0eOPP16o96NH+gs90vXRIwGFw5lSgJ0qXbq01fO9e/fqzTff1N69e+Xh4aE6deqoatWqknTD6/wlWU71vurql2R+DUh+62RmZio3N1cVKlSwWsbZ2dlyROZmSpUqpVatWll+0ebEiROaOHGiNm7cqP/85z9q27atMjMztWHDBm3YsOGa9cuXL1+g95GkKlWqyGQyWf2k7t+dPXtWLi4uKlOmzDVzFy9e1IQJE/Thhx/q8uXL8vPzU0BAgFxcXCx/94CAAMXHx2vx4sVatGiR4uPj5ePjo2HDhqlv374qU6aMli9frtmzZ+vTTz/VqlWr5O7urscff1yvvvqq5chhYZ04ceKGDVuXLl2Ul5en9957T7NmzVJMTIyqVaumkSNHqkuXLvm+7t/3vevx9fW1en51fzh37lwBq7+5s2fPXvM+0pWjn39/r1vZzwEAxd+5c+eUl5enefPmad68edfMu7m5SZIGDx6sMmXKKCEhQVOmTFF0dLTq1q2rV199VS1btizw+9Ej/YUe6frokYDCIZQCioGr19vXq1dPn3zyiWrVqiUnJydt3rxZGzduNLSWChUqqFSpUpYbcl51tRnLT8+ePXXPPfdc8/PElSpV0qRJk/T5558rNTVVbdu2VdmyZfXAAw/o2WefveZ1CnK08apy5cqpUaNG+vbbbzVq1KjrHjWLjY3VypUr9fXXX18zN2nSJG3cuFHTp0/XAw88YGlG7r//fqvlrjaQWVlZ2r59u5YuXaqJEyfK399fTZs2Va1atRQdHa3c3Fzt2bNHH374oVasWKG7775bgwcPLvD2XHX27Fn9+OOP+R7hDQkJUUhIiP744w9t2bJF8+bN06hRoxQYGGg5snur/v7fOj09XZKsGvG/37fswoULhXqPu+66S7/99ts141ffi1POAQBlypSRyWTSgAED1LVr12vmr/4fcicnJ/Xp00d9+vTRqVOntHnzZs2ZM0fPP/+8vvvuu1sOP/4XPRI9kkSPBBQWl+8BxcChQ4eUmZmpfv36qU6dOpYjHN98840kY490ODs7q3nz5tq0aZPV+FdffaXLly/nu261atX02WefWX4d5n/98ssvkmS51v7qr5o0aNBATZo0UZMmTdS4cWMtXrxYX3zxhaWWghg0aJB+/vlnLVu27Jq51NRUJSQk6IEHHrAcXfpfiYmJatGihdq3b29ptvbt26fTp09b/u5vv/22unfvLrPZLA8PD7Vt29ZyA8mjR4/qs88+U8uWLZWeni5nZ2cFBARo3Lhx8vLy0tGjRwu0DX83Z84cXbp0SU8//fR151944QWFhYVJunLDz86dO2v48OG6fPmyTp48KUlWp5YX1tV976pPPvlEJpNJwcHBkiRPT08dP37capm/33D0Zu9/33336ciRI/rhhx+sxj/66COVKlVKTZs2vdXyAQAlhKenpxo2bKhDhw5Z+oUmTZqobt26iomJ0Y4dOyRdCX0mTpwo6Uo4EBoaqj59+ujcuXOWX127ne9FiR5JokeS6JGAwuJMKaAYuOeee+Tp6ak5c+bIxcVFLi4u2rhxo+VnX42+JjwiIkJ9+/ZVRESEevTooaNHj2rGjBmSdMPr9yXpxRdf1I4dO9SjRw/169dPAQEBcnJy0t69e7Vw4UI9/PDDevjhhyVJw4cPV8+ePTV06FD16tVLbm5uWrVqlb788kvNnDlTkiw3v9y2bZtq164tf3//675vly5dtHXrVk2cOFHJycnq1KmTSpcurT179mjRokUqV66cpVH9u6ZNm+rTTz/VihUrVLt2bR04cECzZ8+WyWSy/N1btmypRYsW6ZVXXtFjjz2mS5cuaf78+fL29lbLli2Vk5OjvLw8hYWFaciQISpTpow+/fRT/fHHH5b7DNzI6dOnLT+LnJubq1OnTmnjxo1av369hg0bpiZNmlx3vZYtW+qNN97Q22+/rYcffljnzp1TbGysatasqfr160u6cuPL/fv3a+fOnYVuXvbu3auxY8cqJCREe/fu1cyZM9WjRw/VrFlTktS2bVvNnTtXc+fOlb+/v7766itt377d6jW8vLwkSV988YUefvjha37FJjQ0VO+9957CwsIUEREhPz8/ffXVV0pISFB4eLhlfQCAY3vppZc0ZMgQjRgxQo899pjlV9KSk5Mtv1x23333aeHChfLx8VFAQIBOnDihRYsWKTg42HLJm5eXl3bv3q1du3YpKCjolmqhR6JHokcCCodQCigGypYtq1mzZmny5Mn617/+pTJlyqhBgwZatmyZnnvuOX3//fdq166dYfUEBQUpJiZGM2bM0PDhw1WtWjW99tprevHFF697z4Gr/Pz8tG7dOs2dO1cff/yx5s2bJ7PZrBo1amjQoEHq16+fpWGrX7++li9frmnTpmn06NEym8269957FRcXp0ceeUTSlSNNzz77rFatWqXNmzfru+++U6lSpa773hMnTlSLFi30/vvv6/XXX9eff/6pqlWr6sknn9SgQYNueJrzK6+8okuXLmn69OnKycmRn5+f/vnPfyo1NVVfffWVcnNz1bp1a02ZMkULFy603LgzMDBQS5cutdxDYv78+ZoxY4bGjh2rrKwsyxHcm93HYvPmzdq8ebOkK82sl5eXGjZsqJkzZ6pjx443XK9nz566dOmSVq5cqffee0/u7u66//77NWrUKMvfaODAgXrrrbc0aNAgLVq0KN86/i4sLEz79u3TsGHDVLZsWQ0ePFjh4eGW+aFDh+r06dNasGCBLl26pDZt2mjSpEmWnziWrtx09oEHHtDUqVO1bds2xcfHW72Hh4eH3n33XU2dOlUzZszQ+fPnVatWLU2aNEk9evQoVL0AgJLroYce0oIFCxQbG6uIiAiVKlVKjRo10qJFiyy/nvavf/1Lrq6uSkhIUFxcnMqWLat27dppxIgRltcZNmyYZs2apeeee+6692sqCHokeiR6JKBwTOb87pAMANexadMmVa5cWY0aNbKMpaSkKCQkRLNmzbI0RAAAAI6EHgkACoczpQAU2pYtW7RhwwaNHDlS99xzj06cOKHZs2erVq1aeuihh2xdHgAAgE3QIwFA4XCmFIBCu3jxombMmKGNGzfq5MmT8vb2VqtWrTRixIjr3ggTAADAEdAjAUDhEEoBAAAAAADAcLf3u6cAAAAAAADALSCUAgAAAAAAgOEIpQAAAAAAAGA4QikAAAAAAAAYjlAKAAAAAAAAhnOxdQH27NSpP8RvEwIAgKtMJqlChbK2LsPu0DMBAID/VdCeiVAqH2azaLAAAABugp4JAADcCi7fAwAAAAAAgOEIpQAAAAAAAGA4uwilcnJyFBISoh07dlwz98cff6hVq1Zau3at1fj69evVvn17+fv7KywsTKdPn7bMmc1mTZkyRS1btlRwcLAmT56svLy8It8OAAAAW/jtt980aNAgBQQEqE2bNpo/f75lbuLEiapXr57VY9myZZb5/HoqAACAomTzUCo7O1svvfSSUlJSrjsfHR2tkydPWo3t2bNHY8eOVXh4uFatWqVz584pMjLSMr9o0SKtX79esbGxmjlzpj7++GMtWrSoSLcDAADAFvLy8jRkyBCVK1dO69at05tvvqnZs2fr448/liQdPHhQI0aM0JYtWyyP7t27S7p5TwUAAFCUbBpKpaam6qmnntLhw4evO//9999r+/bt8vX1tRpftmyZOnfurG7duql+/fqaPHmyNm/erLS0NEnS0qVLFRERoaCgILVs2VIjR47U8uXLi3x7AAAAjJaRkaEGDRpo3Lhxqlmzplq3bq37779fiYmJkq6EUg0bNpSvr6/l4eHhIenmPRUAAEBRsmkotXPnTrVo0UKrVq26Zi4nJ0evvfaaXn/9dbm6ulrNJScnKygoyPK8SpUqqlq1qpKTk3XixAkdO3ZM9913n2U+MDBQR44cueaMKwAAgOKuYsWKmj59ujw9PWU2m5WYmKhdu3YpODhY58+f14kTJ1SzZs3rrptfTwUAAFDUXGz55r17977h3Jw5c9SwYUM99NBD18ydPHlSFStWtBqrUKGCjh8/rvT0dEmymvfx8ZEkHT9+/Jr18mMyFXhRAADgAOy9N2jXrp2OHj2qtm3bqmPHjtq3b59MJpPmzJmjb775Rt7e3nr22Wf1xBNPSMq/pwIAAChqNg2lbiQ1NVUrV67URx99dN35ixcvXnP2lKurq3JycnTx4kXL8/+dk66cfVUYFSqULdTyAAAAtjRz5kxlZGRo3LhxioqKUqNGjWQymVSrVi0988wz2rVrl1577TV5enqqQ4cO+fZUhWHvYR0AADBWQXsDuwulzGazXn31VUVERFjOcPo7Nze3a5qlnJwceXh4WAVQbm5uln+XZLl/QkGdOvWHzObCbgEAACipTCb7PmjVpEkTSVd+SGbkyJHavXu32rZtK29vb0lS/fr19euvv2rFihXq0KFDvj1VYdjz3wQAANgvuwuljh49qh9++EE//fST3n77bUlSVlaW3njjDW3YsEHz589XpUqVlJGRYbVeRkaGfH19ValSJUlSenq6/Pz8LP8u6Zobpt+M2SxCKQAAYNcyMjKUlJSk9u3bW8bq1KmjS5cu6fz58ypfvrzV8rVq1dL27dslKd+eqjA4kAcAAP5XQQ/k2V0oValSJX3++edWY3379lXfvn312GOPSZL8/f2VmJio0NBQSdKxY8d07Ngx+fv7q1KlSqpataoSExMtoVRiYqKqVq1aqPtJAQAAFAe///67wsPDtXnzZsvBuX379ql8+fJ699139cMPP2jx4sWW5Q8cOKBatWpJyr+nKgwO5AEAgFthd6GUi4uLatSocc1YhQoVLI1Wr1691LdvXzVr1kxNmjTRpEmT1KZNG1WvXt0yP2XKFFWuXFmSNHXqVA0cONDYDQEAADBAkyZN1KhRI40ZM0aRkZE6cuSIoqOjNWzYMAUEBCg+Pl4LFixQhw4dtGXLFn3wwQdaunSppJv3VAAAAEXJ7kKpgggICND48eM1c+ZMnT17Vg8++KAmTJhgmR80aJBOnTql8PBwOTs7q0ePHhowYIDtCgYAACgizs7OmjVrliZMmKCnn35aHh4e6tu3r/r16yeTyaQZM2Zo5syZmjFjhqpVq6apU6cqICBA0s17KgAAgKJkMps52fpGMjK4PwIAAPiLyST5+HBT77+jZwIAAP+roD2TkwG1AAAAAAAAAFYIpQAAAAAAAGC4YnlPKQBFx8nJJCcnk63LQDGQl2dWXh7X6wAAHBM9EwqKngm4MUIpABZOTiZ5e5eWszMnUeLmcnPzlJl5gSYLAOBw6JlQGPRMwI0RSgGwcHIyydnZSWM+SNChjAxblwM7VsvHR2916y4nJxMNFgDA4dAzoaDomYD8EUoBuMahjAwdOH7M1mUAAADYNXomALg9nG8KAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHB2EUrl5OQoJCREO3bssIwlJSWpZ8+eCggIUMeOHbV69WqrdbZu3aqQkBD5+/urX79+SktLs5pfvHixWrVqpYCAAI0ZM0ZZWVmGbAsAAAAAAABuzuahVHZ2tl566SWlpKRYxtLT0/Xcc88pODhY69atU0REhCZMmKD//Oc/kqSjR48qLCxMoaGhWrNmjcqXL6/hw4fLbDZLkjZu3KjY2FiNHz9eS5YsUXJysqKjo22xeQAAAEXut99+06BBgxQQEKA2bdpo/vz5lrm0tDQNGDBAzZo1U5cuXbRlyxardW92oA8AAKCo2DSUSk1N1VNPPaXDhw9bjX/55Zfy8fHRSy+9pJo1a6pr167q1q2bPv74Y0nS6tWr1bhxYw0cOFB169ZVVFSUjhw5op07d0qSli5dqv79+6tt27Zq2rSp3nzzTSUkJHC2FAAAKHHy8vI0ZMgQlStXTuvWrdObb76p2bNn6+OPP5bZbFZYWJh8fHyUkJCgxx9/XOHh4Tp69Kikmx/oAwAAKEo2DaV27typFi1aaNWqVVbjrVq1UlRU1DXLnz9/XpKUnJysoKAgy7iHh4caNWqkpKQk5ebmau/evVbzzZo106VLl3TgwIEi2hIAAADbyMjIUIMGDTRu3DjVrFlTrVu31v3336/ExERt375daWlpGj9+vGrXrq2hQ4eqWbNmSkhIkHTzA30AAABFyaahVO/evTVmzBh5eHhYjfv5+alZs2aW56dOndInn3yi+++/X9KVy/sqVqxotU6FChV0/PhxnTt3TtnZ2VbzLi4u8vb21vHjx4tuYwAAAGygYsWKmj59ujw9PWU2m5WYmKhdu3YpODhYycnJatiwoUqXLm1ZPjAwUElJSZLyP9AHAABQ1FxsXcDNXLx4Uc8//7x8fHz09NNPS5KysrLk6upqtZyrq6tycnJ08eJFy/PrzReGyXQbhQOAg+CzEo7E3vf3du3a6ejRo2rbtq06duyot95664YH8qT8D/QVhr3/XQDAHvBZCUdS0P3drkOpP//8U8OHD9evv/6q9957z3JGlZub2zUBU05Ojry8vOTm5mZ5/vf5v5+RdTMVKpS9jeoBoOQrV66MrUsA8D9mzpypjIwMjRs3TlFRUfkeyJPyP9BXGPRMAJA/eibg+uw2lDp//rwGDx6sw4cPa8mSJapZs6ZlrlKlSsrIyLBa/ur9FLy9veXm5qaMjAzVrl1bknT58mVlZmbK19e3UDWcOvWHuM8nHImzsxNfmCiUM2f+VG5unq3LAAxjMtl3ANOkSRNJV37deOTIkerevfs1P/SSk5Mjd3d3Sfkf6CsMeiY4GnomFBY9ExxNQXsmuwyl8vLyFB4ert9//13vvvuuJVy6yt/fX4mJiZbnWVlZ2r9/v8LDw+Xk5KQmTZooMTFRLVq0kCQlJSXJxcVF9evXL1QdZrNosADgJvicBGwrIyNDSUlJat++vWWsTp06unTpknx9fXXo0KFrlr96yV5+B/oKg54JAG6Oz0ngWja90fmNrFmzRjt27NDEiRPl5eWl9PR0paenKzMzU5LUvXt37d69W/Hx8UpJSVFkZKT8/PwsIVTv3r21YMECffnll9qzZ4/GjRunp556qtCX7wEAANi733//XeHh4Tpx4oRlbN++fSpfvrwCAwP1448/Wu65KUmJiYny9/eXdOMDfVfnAQAAipJdnim1ceNG5eXlaejQoVbjwcHBevfdd+Xn56eYmBi99dZbiouLU0BAgOLi4mT67520unbtqiNHjuj1119XTk6OHn30UY0aNcoWmwIAAFCkmjRpokaNGmnMmDGKjIzUkSNHFB0drWHDhik4OFhVqlRRZGSkhg8frq+//lp79uxRVFSUpCsH+hYsWKD4+Hi1bdtWcXFxVgf6AAAAipLJbOYkwhvJyOD+CHAsLi5X7o/Qc/5cHTh+zNblwI7Vr1xFKwcP1Zkzf+ryZe6PAMdhMkk+PvZ3T6kTJ05owoQJ2rZtmzw8PPTMM89o6NChMplM+u233zR27FglJyerRo0aGjNmjB544AHLups3b9Zbb72l48ePKyAgQBMmTFD16tUL9f70THA09EwoKHomOKqC9kx2eaYUAAAACq5SpUqKjY297lyNGjW0bNmyG67bunVrtW7duqhKAwAAuCG7vKcUAAAAAAAASjZCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4QilAAAAAAAAYDhCKQAAAAAAABiOUAoAAAAAAACGI5QCAAAAAACA4ewilMrJyVFISIh27NhhGUtLS9OAAQPUrFkzdenSRVu2bLFaZ+vWrQoJCZG/v7/69euntLQ0q/nFixerVatWCggI0JgxY5SVlWXItgAAABjpxIkTioiIUHBwsFq1aqWoqChlZ2dLkiZOnKh69epZPZYtW2ZZd/369Wrfvr38/f0VFham06dP22ozAACAA7J5KJWdna2XXnpJKSkpljGz2aywsDD5+PgoISFBjz/+uMLDw3X06FFJ0tGjRxUWFqbQ0FCtWbNG5cuX1/Dhw2U2myVJGzduVGxsrMaPH68lS5YoOTlZ0dHRNtk+AACAomI2mxUREaGsrCwtX75c06ZN09dff63p06dLkg4ePKgRI0Zoy5Ytlkf37t0lSXv27NHYsWMVHh6uVatW6dy5c4qMjLTh1gAAAEdj01AqNTVVTz31lA4fPmw1vn37dqWlpWn8+PGqXbu2hg4dqmbNmikhIUGStHr1ajVu3FgDBw5U3bp1FRUVpSNHjmjnzp2SpKVLl6p///5q27atmjZtqjfffFMJCQmcLQUAAEqUQ4cOKSkpSVFRUapbt66CgoIUERGh9evXS7oSSjVs2FC+vr6Wh4eHhyRp2bJl6ty5s7p166b69etr8uTJ2rx58zVnnwMAABQVm4ZSO3fuVIsWLbRq1Sqr8eTkZDVs2FClS5e2jAUGBiopKckyHxQUZJnz8PBQo0aNlJSUpNzcXO3du9dqvlmzZrp06ZIOHDhQtBsEAABgIF9fX82fP18+Pj5W4+fPn9f58+d14sQJ1axZ87rr/r2fqlKliqpWrark5OSiLBkAAMDCxZZv3rt37+uOp6enq2LFilZjFSpU0PHjx286f+7cOWVnZ1vNu7i4yNvb27J+QZlMhVocABwSn5VwJPa2v3t5ealVq1aW53l5eVq2bJlatmypgwcPymQyac6cOfrmm2/k7e2tZ599Vk888YQk6eTJk/n2WwAAAEXNpqHUjWRlZcnV1dVqzNXVVTk5OTedv3jxouX5jdYvqAoVyha2dABwKOXKlbF1CQD+R3R0tPbv3681a9boxx9/lMlkUq1atfTMM89o165deu211+Tp6akOHTro4sWLd6RfkuwvrAMAe8RnJRxJQfd3uwyl3NzclJmZaTWWk5Mjd3d3y/zfG6acnBx5eXnJzc3N8vzv81fvoVBQp079of/eOx1wCM7OToQMKJQzZ/5Ubm6ercsADGMy2e9Bq+joaC1ZskTTpk3Tvffeq7p166pt27by9vaWJNWvX1+//vqrVqxYoQ4dOtywnypsvyTZ798EAOwFPTZwfXYZSlWqVEmpqalWYxkZGZZTzCtVqqSMjIxr5hs0aCBvb2+5ubkpIyNDtWvXliRdvnxZmZmZ8vX1LVQdZrMIpQDgJvicBGxvwoQJWrFihaKjo9WxY0dJkslksgRSV9WqVUvbt2+XdON+qrD9ksSBPDgeDuShsDiQB0dT0AN5Nr3R+Y34+/vrxx9/tFyKJ0mJiYny9/e3zCcmJlrmsrKytH//fvn7+8vJyUlNmjSxmk9KSpKLi4vq169v3EYAAAAYIDY2VitXrtQ777yjrl27WsZnzJihAQMGWC174MAB1apVS9K1/dSxY8d07NgxS79VGFcP5PHg4SgP4FbYer/lwcPoR0HYZSgVHBysKlWqKDIyUikpKYqPj9eePXvUo0cPSVL37t21e/duxcfHKyUlRZGRkfLz81OLFi0kXbmB+oIFC/Tll19qz549GjdunJ566qlbOh0dAADAXh08eFCzZs3Sc889p8DAQKWnp1sebdu21a5du7RgwQIdPnxY7733nj744AMNHDhQktSrVy99+OGHWr16tQ4cOKDRo0erTZs2ql69uo23CgAAOAq7vHzP2dlZs2bN0tixYxUaGqoaNWooLi5OVatWlST5+fkpJiZGb731luLi4hQQEKC4uDiZ/nsnra5du+rIkSN6/fXXlZOTo0cffVSjRo2y5SYBAADccZs2bVJubq5mz56t2bNnW8399NNPmjFjhmbOnKkZM2aoWrVqmjp1qgICAiRJAQEBGj9+vGbOnKmzZ8/qwQcf1IQJE2yxGQAAwEGZzOaCnlTleDIyuD8CHIuLy5X7I/ScP1cHjh+zdTmwY/UrV9HKwUN15syfunyZ+yPAcZhMko8PN/X+O3omOBp6JhQUPRMcVUF7Jru8fA8AAAAAAAAlG6EUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwnF2HUseOHdPQoUPVvHlztWvXTosXL7bM7d+/X08++aT8/f3VvXt37du3z2rd9evXq3379vL391dYWJhOnz5tcPUAAABF78SJE4qIiFBwcLBatWqlqKgoZWdnS5LS0tI0YMAANWvWTF26dNGWLVus1t26datCQkLk7++vfv36KS0tzRabAAAAHJRdh1IvvPCCSpcurbVr12rMmDGaPn26vvjiC124cEFDhgxRUFCQ1q5dq4CAAA0dOlQXLlyQJO3Zs0djx45VeHi4Vq1apXPnzikyMtLGWwMAAHBnmc1mRUREKCsrS8uXL9e0adP09ddfa/r06TKbzQoLC5OPj48SEhL0+OOPKzw8XEePHpUkHT16VGFhYQoNDdWaNWtUvnx5DR8+XGaz2cZbBQAAHIWLrQu4kbNnzyopKUkTJkxQzZo1VbNmTbVq1Urbtm3T2bNn5ebmptGjR8tkMmns2LH65ptv9Nlnnyk0NFTLli1T586d1a1bN0nS5MmT1bZtW6Wlpal69eq23TAAAIA75NChQ0pKStJ3330nHx8fSVJERITefvttPfzww0pLS9PKlStVunRp1a5dW9u2bVNCQoKef/55rV69Wo0bN9bAgQMlSVFRUXrwwQe1c+dOtWjRwpabBQAAHITdninl7u4uDw8PrV27VpcuXdKhQ4e0e/duNWjQQMnJyQoMDJTJZJIkmUwmNW/eXElJSZKk5ORkBQUFWV6rSpUqqlq1qpKTk22xKQAAAEXC19dX8+fPtwRSV50/f17Jyclq2LChSpcubRkPDAy8Yb/k4eGhRo0aWeYBAACKmt2eKeXm5qbXX39dEyZM0NKlS5Wbm6vQ0FA9+eST2rRpk+rUqWO1fIUKFZSSkiJJOnnypCpWrHjN/PHjxwtVw38zLwBAPvishCOxt/3dy8tLrVq1sjzPy8vTsmXL1LJlS6Wnp+fbD91svjDs7e8CAPaIz0o4koLu73YbSknSwYMH1bZtWz377LNKSUnRhAkTdP/99ysrK0uurq5Wy7q6uionJ0eSdPHixXznC6pChbK3twEAUMKVK1fG1iUA+B/R0dHav3+/1qxZo8WLF+fbD92snyoMeiYAyB89E3B9dhtKbdu2TWvWrNHmzZvl7u6uJk2a6MSJE5o9e7aqV69+TcOUk5Mjd3d3SVfOsrrevIeHR6FqOHXqD3GvTzgSZ2cnvjBRKGfO/Knc3DxblwEYxmSy3wAmOjpaS5Ys0bRp03TvvffKzc1NmZmZVssUpF/y8vIq9HvTM8HR0DOhsOiZ4GgK2jPZbSi1b98+1ahRw9I4SVLDhg01Z84cBQUFKSMjw2r5jIwMyynolSpVuu68r69voWowm0WDBQA3weckYHsTJkzQihUrFB0drY4dO0q60g+lpqZaLVeQfqlBgwaFfn96JgC4OT4ngWvZ7Y3OK1asqN9++83qCN6hQ4fk5+cnf39//fDDD5afLDabzdq9e7f8/f0lSf7+/kpMTLSsd+zYMR07dswyDwAAUFLExsZq5cqVeuedd9S1a1fLuL+/v3788UddvHjRMpaYmHjDfikrK0v79++nXwIAAIax21CqXbt2KlWqlF599VX98ssv+uqrrzRnzhz17dtXnTp10rlz5zRp0iSlpqZq0qRJysrKUufOnSVJvXr10ocffqjVq1frwIEDGj16tNq0aaPq1avbeKsAAADunIMHD2rWrFl67rnnFBgYqPT0dMsjODhYVapUUWRkpFJSUhQfH689e/aoR48ekqTu3btr9+7dio+PV0pKiiIjI+Xn56cWLVrYeKsAAICjsNtQqmzZslq8eLHS09PVo0cPRUVF6Z///KeefvppeXp6au7cuUpMTFRoaKiSk5MVHx9v+cnjgIAAjR8/XnFxcerVq5fuuusuRUVF2XiLAAAA7qxNmzYpNzdXs2fP1kMPPWT1cHZ21qxZs5Senq7Q0FB99NFHiouLU9WqVSVJfn5+iomJUUJCgnr06KHMzEzFxcXJxM9DAQAAg5jM5jt7Zevp06dVvnz5O/mSNpORwU074VhcXK7ctLPn/Lk6cPyYrcuBHatfuYpWDh6qM2f+1OXL3LQTjsNkknx8iv5G58Wtn6JngqOhZ0JB0TPBURW0Z7qlM6UaNGig06dPXzN+5MgRPfLII7fykgAAAA6FfgoAADi6Av/63gcffKC1a9dKunJj8bCwMJUqVcpqmZMnTxb6F+4AAAAcBf0UAADAXwocSnXo0EG///67JGnnzp1q1qyZypQpY7VM6dKl1aFDhztbIQAAQAlBPwUAAPCXAodSZcqUUXh4uCSpWrVq6tKli9zc3IqsMAAAgJKGfgoAAOAvBQ6l/tcTTzyh3377Tfv27dOlS5eume/Wrdvt1gUAAFCi0U8BAABHd0uh1Pz58zVlyhTddddd15xybjKZaKIAAABugn4KAAA4ulsKpRYuXKhRo0Zp0KBBd7oeAAAAh0A/BQAAHJ3TrayUnZ2tRx999E7XAgAA4DDopwAAgKO7pVDqH//4h9577z2ZzeY7XQ8AAIBDoJ8CAACO7pYu3zt//rzWrFmj9evXy8/PT6VKlbKaX7p06R0pDgAAoKSinwIAAI7ulkKpmjVratiwYXe6FgAAAIdBPwUAABzdLYVS4eHhd7oOAAAAh0I/BQAAHN0thVKRkZH5zkdFRd1SMQAAAI6CfgoAADi6W7rR+d9dvnxZv/zyizZs2KDy5cvfiZcEAABwKPRTAADA0dzSmVI3OnI3f/58/fzzz7dVEAAAgCOgnwIAAI7ujpwpdVWnTp30xRdf3MmXBAAAcCj0UwAAwFHcsVDqwoULev/991WuXLk79ZIAAAAOhX4KAAA4klu6fK9+/foymUzXjLu5uWnixIm3XRQAAEBJRz8FAAAc3S2FUkuXLrV6bjKZVKpUKdWpU0eenp53pDAAAICSjH4KAAA4ulsKpYKDgyVJv/76qw4ePKi8vDzdc889NFAAAAAFRD8FAAAc3S2FUufOnVNkZKQ2bdqku+66S7m5ufrzzz913333KS4uTmXLlr3TdQIAAJQo9FMAAMDR3dKNzidOnKjjx49rw4YN2rFjh77//nt9/PHHunDhwg1/3hgAAAB/oZ8CAACO7pZCqa+++krjxo1TrVq1LGN16tTR66+/rk2bNt2x4gAAAEoq+ikAAODobimUcnNzk5PTtauaTCbl5ubedlEAAAAlHf0UAABwdLcUSrVr105vvvmmDh8+bBn79ddfNXHiRLVu3fqOFQcAAFBS0U8BAABHd0s3Oh81apTCwsLUsWNHeXl5SZLOnj2rhx9+WK+99todLRAAAKAkop8CAACOrtCh1G+//aaqVavq3Xff1U8//aSDBw/Kzc1NNWvWVO3atYuiRgAAgBKFfgoAAKAQl++ZzWZNnDhRnTt31g8//CBJqlevnrp06aKEhASFhITo3//+t8xmc5EVCwAAUJzRTwEAAPylwKHU0qVLtWHDBsXFxSk4ONhqbtasWYqLi9O6deu0YsWKO14kAABASUA/BQAA8JcCh1Lvv/++XnvtNbVt2/a68+3atdPIkSNpogAAAG6AfgoAAOAvBQ6ljhw5oqZNm+a7TMuWLZWWlnbbRQEAAJRE9FMAAAB/KXAoVaFCBR05ciTfZY4fPy5vb+/brQkAAKBEop8CAAD4S4FDqQ4dOigmJkaXLl267vzly5cVGxurhx566I4VBwAAUJLQTwEAAPzFpaALDh8+XD169FBoaKj69u2rxo0bq2zZsjp79qx+/PFHLVu2TH/++acmT55clPUCAAAUW/RTAAAAfylwKOXl5aX3339fU6ZM0b///W9lZWVJuvLTxmXLllWXLl30/PPPy8fHp8iKBQAAKM7opwAAAP5S4FBKkry9vTVx4kS9/vrrSktL07lz5+Tt7a27775bzs7ORVUjAABAiUE/BQAAcEWhQqmrXF1dVbt27TtdCwAAgMOgnwIAAI6uwDc6BwAAAAAAAO4UQikAAAAAAAAYjlAKAAAAAAAAhiOUAgAAAAAAgOEIpQAAAAAAAGA4QikAAAAAAAAYjlAKAAAAAAAAhrPrUConJ0dvvvmm7rvvPj3wwAN65513ZDabJUn79+/Xk08+KX9/f3Xv3l379u2zWnf9+vVq3769/P39FRYWptOnT9tiEwAAAAAAAHAddh1KTZw4UVu3btWCBQs0depUvf/++1q1apUuXLigIUOGKCgoSGvXrlVAQICGDh2qCxcuSJL27NmjsWPHKjw8XKtWrdK5c+cUGRlp460BAAAoWjk5OQoJCdGOHTssYxMnTlS9evWsHsuWLbPMcyAPAADYioutC7iRzMxMJSQkaNGiRWratKkkaeDAgUpOTpaLi4vc3Nw0evRomUwmjR07Vt98840+++wzhYaGatmyZercubO6desmSZo8ebLatm2rtLQ0Va9e3YZbBQAAUDSys7M1YsQIpaSkWI0fPHhQI0aM0BNPPGEZ8/T0lPTXgbw333xT9evX16RJkxQZGam5c+caWjsAAHBMdnumVGJiojw9PRUcHGwZGzJkiKKiopScnKzAwECZTCZJkslkUvPmzZWUlCRJSk5OVlBQkGW9KlWqqGrVqkpOTjZ0GwAAAIyQmpqqp556SocPH75m7uDBg2rYsKF8fX0tDw8PD0myOpBXv359TZ48WZs3b1ZaWprRmwAAAByQ3YZSaWlpqlatmj744AN16tRJjzzyiOLi4pSXl6f09HRVrFjRavkKFSro+PHjkqSTJ0/mOw8AAFCS7Ny5Uy1atNCqVausxs+fP68TJ06oZs2a112PA3kAAMCW7PbyvQsXLui3337TypUrFRUVpfT0dL3++uvy8PBQVlaWXF1drZZ3dXVVTk6OJOnixYv5zhfUf0/EAgDkg89KOBJ73d979+593fGDBw/KZDJpzpw5+uabb+Tt7a1nn33WcinfnTqQZ69/FwCwJ3xWwpEUdH+321DKxcVF58+f19SpU1WtWjVJ0tGjR7VixQrVqFHjmoApJydH7u7ukiQ3N7frzl89Vb2gKlQoextbAAAlX7lyZWxdAoB8HDp0SCaTSbVq1dIzzzyjXbt26bXXXpOnp6c6dOhwxw7k0TMBQP7omYDrs9tQytfXV25ubpZASpLuueceHTt2TMHBwcrIyLBaPiMjw3Kkr1KlSted9/X1LVQNp079IbP5FjcAKIacnZ34wkShnDnzp3Jz82xdBmAYk6l4BTDdunVT27Zt5e3tLUmqX7++fv31V61YsUIdOnS4Ywfy6JngaOiZUFj0THA0Be2Z7DaU8vf3V3Z2tn755Rfdc889kq4c7atWrZr8/f01b948mc1mmUwmmc1m7d69W8OGDbOsm5iYqNDQUEnSsWPHdOzYMfn7+xeqBrNZNFgAcBN8TgL2y2QyWQKpq2rVqqXt27dLunMH8uiZAODm+JwErmW3NzqvVauW2rRpo8jISB04cEDffvut4uPj1atXL3Xq1Ennzp3TpEmTlJqaqkmTJikrK0udO3eWJPXq1UsffvihVq9erQMHDmj06NFq06aNqlevbuOtAgAAMM6MGTM0YMAAq7EDBw6oVq1akv46kHfVrR7IAwAAuBV2G0pJ0pQpU3T33XerV69eevnll9WnTx/17dtXnp6emjt3ruVsqOTkZMXHx6t06dKSpICAAI0fP15xcXHq1auX7rrrLkVFRdl4awAAAIzVtm1b7dq1SwsWLNDhw4f13nvv6YMPPtDAgQMlcSAPAADYlt1evidJZcuW1eTJk68717RpU61bt+6G64aGhlou3wMAAHBETZs21YwZMzRz5kzNmDFD1apV09SpUxUQECDprwN5M2fO1NmzZ/Xggw9qwoQJNq4aAAA4CrsOpQAAAFA4P/30k9Xz9u3bq3379jdcngN5AADAVuz68j0AAAAAAACUTIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDudi6AAAAbpeTk0lOTiZbl4FiIC/PrLw8s63LAADAJuiZUFBG9UyEUgCAYs3JySTvch5ydnK2dSkoBnLzcpV5JotgCgDgcOiZUBhG9UyEUgCAYs3JySRnJ2ct2bdUxy+csHU5sGOVS1dS/8b95ORkIpQCADgceiYUlJE9E6EUAKBEOH7hhH7/43dblwEAAGDX6JlgT7jROQAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMFyxCaWGDBmiV155xfJ8//79evLJJ+Xv76/u3btr3759VsuvX79e7du3l7+/v8LCwnT69GmjSwYAADBMTk6OQkJCtGPHDstYWlqaBgwYoGbNmqlLly7asmWL1Tpbt25VSEiI/P391a9fP6WlpRldNgAAcGDFIpT65JNPtHnzZsvzCxcuaMiQIQoKCtLatWsVEBCgoUOH6sKFC5KkPXv2aOzYsQoPD9eqVat07tw5RUZG2qp8AACAIpWdna2XXnpJKSkpljGz2aywsDD5+PgoISFBjz/+uMLDw3X06FFJ0tGjRxUWFqbQ0FCtWbNG5cuX1/Dhw2U2F+1PPwMAAFxl96FUZmamJk+erCZNmljGNmzYIDc3N40ePVq1a9fW2LFjVaZMGX322WeSpGXLlqlz587q1q2b6tevr8mTJ2vz5s0c/QMAACVOamqqnnrqKR0+fNhqfPv27UpLS9P48eNVu3ZtDR06VM2aNVNCQoIkafXq1WrcuLEGDhyounXrKioqSkeOHNHOnTttsRkAAMAB2X0o9fbbb+vxxx9XnTp1LGPJyckKDAyUyWSSJJlMJjVv3lxJSUmW+aCgIMvyVapUUdWqVZWcnGxo7QAAAEVt586datGihVatWmU1npycrIYNG6p06dKWscDAwBv2Sx4eHmrUqJFlHgAAoKi52LqA/Gzbtk3ff/+9Pv74Y40bN84ynp6ebhVSSVKFChUsp6yfPHlSFStWvGb++PHjhXr//2ZeAIB88FmJ4uhW91t73N979+593fH09PR8+6GbzQMAABQ1uw2lsrOz9cYbb+j111+Xu7u71VxWVpZcXV2txlxdXZWTkyNJunjxYr7zBVWhQtlbqBwAHEe5cmVsXQJQaI6y396sX7rZfGHYY1gHAPaGz0oUR0V9IM9uQ6nY2Fg1btxYrVq1umbOzc3tmoYpJyfHEl7daN7Dw6NQNZw69Ye41yccibOzk8P8nzXcGWfO/Knc3Dyb1sB+i8K6nf3WZCo+B63c3NyUmZlpNVaQfsnLy6vQ71Vc/iYAYCv0KiiOjNhv7TaU+uSTT5SRkaGAgABJsjRNGzduVEhIiDIyMqyWz8jIsJyCXqlSpevO+/r6FqoGs1mEUgBwE3xOojhyhP22UqVKSk1NtRorSL/UoEGDQr8XB/LgaDgggsLiQB6KIyMO5NltKPXuu+/q8uXLludTpkyRJI0cOVK7du3SvHnzZDabZTKZZDabtXv3bg0bNkyS5O/vr8TERIWGhkqSjh07pmPHjsnf39/4DQEAALABf39/xcfH6+LFi5azoxITExUYGGiZT0xMtCyflZWl/fv3Kzw8vNDvxYE8ALg5PidRHBX1fmu3v75XrVo11ahRw/IoU6aMypQpoxo1aqhTp046d+6cJk2apNTUVE2aNElZWVnq3LmzJKlXr1768MMPtXr1ah04cECjR49WmzZtVL16dRtvFQAAgDGCg4NVpUoVRUZGKiUlRfHx8dqzZ4969OghSerevbt2796t+Ph4paSkKDIyUn5+fmrRooWNKwcAAI7CbkOp/Hh6emru3LmWs6GSk5MVHx9v+cnjgIAAjR8/XnFxcerVq5fuuusuRUVF2bhqAAAA4zg7O2vWrFlKT09XaGioPvroI8XFxalq1aqSJD8/P8XExCghIUE9evRQZmam4uLiZOJOvAAAwCB2e/ne3/373/+2et60aVOtW7fuhsuHhoZaLt8DAABwBD/99JPV8xo1amjZsmU3XL5169Zq3bp1UZcFAABwXcXyTCkAAAAAAAAUb4RSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcHYdSp04cUIREREKDg5Wq1atFBUVpezsbElSWlqaBgwYoGbNmqlLly7asmWL1bpbt25VSEiI/P391a9fP6WlpdliEwAAAGzqiy++UL169aweERERkqT9+/frySeflL+/v7p37659+/bZuFoAAOBI7DaUMpvNioiIUFZWlpYvX65p06bp66+/1vTp02U2mxUWFiYfHx8lJCTo8ccfV3h4uI4ePSpJOnr0qMLCwhQaGqo1a9aofPnyGj58uMxms423CgAAwFipqalq27attmzZYnlMnDhRFy5c0JAhQxQUFKS1a9cqICBAQ4cO1YULF2xdMgAAcBB2G0odOnRISUlJioqKUt26dRUUFKSIiAitX79e27dvV1pamsaPH6/atWtr6NChatasmRISEiRJq1evVuPGjTVw4EDVrVtXUVFROnLkiHbu3GnjrQIAADDWwYMHde+998rX19fy8PLy0oYNG+Tm5qbRo0erdu3aGjt2rMqUKaPPPvvM1iUDAAAHYbehlK+vr+bPny8fHx+r8fPnzys5OVkNGzZU6dKlLeOBgYFKSkqSJCUnJysoKMgy5+HhoUaNGlnmAQAAHMXBgwdVs2bNa8aTk5MVGBgok8kkSTKZTGrevDn9EgAAMIyLrQu4ES8vL7Vq1cryPC8vT8uWLVPLli2Vnp6uihUrWi1foUIFHT9+XJJuOl9Q/+3RAAD54LMSxdGt7rfFbX83m8365ZdftGXLFs2dO1e5ubnq1KmTIiIilJ6erjp16lgtX6FCBaWkpBT6fYrb3wUAbIHPShRHRd0z2W0o9XfR0dHav3+/1qxZo8WLF8vV1dVq3tXVVTk5OZKkrKysfOcLqkKFsrdXNACUcOXKlbF1CUChOdJ+e/ToUUtfNH36dP3++++aOHGiLl68eMf6JYmeCQBuxpG+e1ByGLHfFotQKjo6WkuWLNG0adN07733ys3NTZmZmVbL5OTkyN3dXZLk5uZ2TUOVk5MjLy+vQr3vqVN/iHujw5E4OzvxhYlCOXPmT+Xm5tm0BvZbFNbt7LcmU/EKYKpVq6YdO3borrvukslkUoMGDZSXl6dRo0YpODj4uv3S1X6qMOiZ4Gj47kFh0TOhODKiZ7L7UGrChAlasWKFoqOj1bFjR0lSpUqVlJqaarVcRkaG5ZK9SpUqKSMj45r5Bg0aFOq9zWbRYAHATfA5ieLIkfZbb29vq+e1a9dWdna2fH19r9sv/f0WCAVBzwQAN8fnJIqjot5v7fZG55IUGxurlStX6p133lHXrl0t4/7+/vrxxx918eJFy1hiYqL8/f0t84mJiZa5rKws7d+/3zIPAADgCL799lu1aNFCWVlZlrH/+7//k7e3twIDA/XDDz/I/N9u02w2a/fu3fRLAADAMHYbSh08eFCzZs3Sc889p8DAQKWnp1sewcHBqlKliiIjI5WSkqL4+Hjt2bNHPXr0kCR1795du3fvVnx8vFJSUhQZGSk/Pz+1aNHCxlsFAABgnICAALm5uenVV1/VoUOHtHnzZk2ePFmDBw9Wp06ddO7cOU2aNEmpqamaNGmSsrKy1LlzZ1uXDQAAHITdhlKbNm1Sbm6uZs+erYceesjq4ezsrFmzZik9PV2hoaH66KOPFBcXp6pVq0qS/Pz8FBMTo4SEBPXo0UOZmZmKi4uz/OQxAACAI/D09NSCBQt0+vRpde/eXWPHjtXTTz+twYMHy9PTU3PnzlViYqJCQ0OVnJys+Ph4lS5d2tZlAwAAB2G395QaMmSIhgwZcsP5GjVqaNmyZTecb926tVq3bl0UpQEAABQbdevW1aJFi64717RpU61bt87gigAAAK6w2zOlAAAAAAAAUHIRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwhFIAAAAAAAAwHKEUAAAAAAAADEcoBQAAAAAAAMMRSgEAAAAAAMBwLrYuoKRzcjLJyclk6zJQDOTlmZWXZ7Z1GQAAGI5+CQVFvwQAJQuhVBFycjLJ27u0nJ05IQ03l5ubp8zMCzRaAACHQr+EwqBfAoCShVCqCDk5meTs7KR/x6zV4SMZti4Hduzuaj565flQOTmZaLIAAA6FfgkFRb8EACUPoZQBDh/JUOovx21dBgAAgN2iXwIAwPFwnjQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAwxFKAQAAAAAAwHCEUgAAAAAAADAcoRQAAAAAAAAMRygFAAAAAAAAw5XYUCo7O1tjxoxRUFCQHnroIS1cuNDWJQEAANgdeiYAAGArLrYuoKhMnjxZ+/bt05IlS3T06FG9/PLLqlq1qjp16mTr0gAAAOwGPRMAALCVEhlKXbhwQatXr9a8efPUqFEjNWrUSCkpKVq+fDkNFgAAwH/RMwEAAFsqkZfvHThwQJcvX1ZAQIBlLDAwUMnJycrLy7NhZQAAAPaDngkAANhSiQyl0tPTVa5cObm6ulrGfHx8lJ2drczMTNsVBgAAYEfomQAAgC2VyMv3srKyrJorSZbnOTk5BX4dJyfJbL79eurUrCx3t1K3/0IosfyqVLD8u5MdRMUNKleWRyn2WdxYzQr2tc9KUnVPP7k6ud58QTisSqUrWv79Vvdbk+kOFWMn7Klnol/CzdhbvyTRM+Hm6JlQHBnZM5XIUMrNze2aRurqc3d39wK/TvnyZe9IPS8Ne+yOvA5KvnLlyti6BEnSGyGP27oEFBP2ss9KUu+GvWxdAooJe9pvbc2eeib6JRSUPf1vmJ4JBWVP+y09EwrKiP3WTrLaO6tSpUo6c+aMLl++bBlLT0+Xu7u7vLy8bFgZAACA/aBnAgAAtlQiQ6kGDRrIxcVFSUlJlrHExEQ1adJETvZyziQAAICN0TMBAABbKpHdhoeHh7p166Zx48Zpz549+vLLL7Vw4UL169fP1qUBAADYDXomAABgSyaz+U7cytv+ZGVlady4cfr888/l6empQYMGacCAAbYuCwAAwK7QMwEAAFspsaEUAAAAAAAA7FeJvHwPAAAAAAAA9o1QCgAAAAAAAIYjlAIAAAAAAIDhCKVgmOzsbI0ZM0ZBQUF66KGHtHDhQluXBBRYTk6OQkJCtGPHDluXAuTrxIkTioiIUHBwsFq1aqWoqChlZ2fbuiwAhUDPhOKKfgnFCT2TfXCxdQFwHJMnT9a+ffu0ZMkSHT16VC+//LKqVq2qTp062bo0IF/Z2dkaMWKEUlJSbF0KkC+z2ayIiAh5eXlp+fLlOnv2rMaMGSMnJye9/PLLti4PQAHRM6E4ol9CcULPZD8IpWCICxcuaPXq1Zo3b54aNWqkRo0aKSUlRcuXL6fBgl1LTU3ViBEjxA+Vojg4dOiQkpKS9N1338nHx0eSFBERobfffpsGCygm6JlQHNEvobihZ7IfXL4HQxw4cECXL19WQECAZSwwMFDJycnKy8uzYWVA/nbu3KkWLVpo1apVti4FuClfX1/Nnz/f0lxddf78eRtVBKCw6JlQHNEvobihZ7IfnCkFQ6Snp6tcuXJydXW1jPn4+Cg7O1uZmZkqX768DasDbqx37962LgEoMC8vL7Vq1cryPC8vT8uWLVPLli1tWBWAwqBnQnFEv4Tihp7JfhBKwRBZWVlWzZUky/OcnBxblAQAJV50dLT279+vNWvW2LoUAAVEzwQAxqNnsh1CKRjCzc3tmkbq6nN3d3dblAQAJVp0dLSWLFmiadOm6d5777V1OQAKiJ4JAIxFz2RbhFIwRKVKlXTmzBldvnxZLi5Xdrv09HS5u7vLy8vLxtUBQMkyYcIErVixQtHR0erYsaOtywFQCPRMAGAceibb40bnMESDBg3k4uKipKQky1hiYqKaNGkiJyd2QwC4U2JjY7Vy5Uq988476tq1q63LAVBI9EwAYAx6JvvANxsM4eHhoW7dumncuHHas2ePvvzySy1cuFD9+vWzdWkAUGIcPHhQs2bN0nPPPafAwEClp6dbHgCKB3omACh69Ez2g8v3YJjIyEiNGzdO/fv3l6enp55//nk9+uijti4LAEqMTZs2KTc3V7Nnz9bs2bOt5n766ScbVQWgsOiZAKBo0TPZD5PZbDbbuggAAAAAAAA4Fi7fAwAAAAAAgOEIpQAAAAAAAGA4QikAAAAAAAAYjlAKAAAAAAAAhiOUAgAAAAAAgOEIpQAAAAAAAGA4QikAAAAAAAAYjlAKAAAAAAAAhiOUAlCitGvXTvXq1bM8GjVqpE6dOmnx4sUFWr9evXrasWPHLb13TEyM+vbta3n+6aef6tSpU7f0WgAAAEWFfgmAvXCxdQEAcKeNGTNGXbp0kSRdvnxZ27dv19ixY+Xt7a1u3boV2fsOHDjQ0mQdOXJEL7zwgjZt2lRk7wcAAHCr6JcA2APOlAJQ4pQtW1a+vr7y9fVVlSpV9MQTT+j+++/X559/XqTvW6ZMGXl7e0uSzGZzkb4XAADA7aBfAmAPCKUAOAQXFxeVKlVKeXl5mj9/vh555BE1bdpUffv21U8//XTddU6cOKGIiAjdd999aty4sZ544gklJiZKkn7//XfVq1dPcXFxuu+++zR+/Hir09EfeeQRyz/fe+89NW/e3KrJu3Tpklq0aKFt27YV8ZYDAAAUDP0SAKMRSgEo0S5duqTPP/9c3333nR555BHFxcVp4cKFGjNmjNatW6dq1app8ODBunDhwjXrjhw5Urm5uVq5cqU++OADVapUSePGjbNaZvfu3UpISFC/fv2sxlevXm35Z2hoqNq3b6+NGzda5rdu3SoXFxcFBwff+Y0GAAAoBPolALbCPaUAlDhvvPGGJkyYIEm6ePGi3N3d1b9/f/3jH/9Qy5Yt9dJLL1mOzE2YMEEdOnTQRx99pJ49e1pew2w2q3379urYsaMqV64sSerTp4+GDBli9V79+/fX3XfffU0N5cuXt/zT3d1dXbt21Ysvvqjs7Gy5ubnps88+U6dOneTs7FwkfwMAAID80C8BsAeEUgBKnIiICD366KOSJDc3N/n6+srZ2VkZGRnKzMyUv7+/ZdlSpUqpcePGOnjwoNVrmEwm9erVSxs2bNDu3bv1yy+/aN++fcrLy7Narlq1agWq6cEHH5Srq6u+/fZbtW7dWl9++aXmzJlzm1sKAABwa+iXANgDQikAJU6FChVUo0aNa8bd3Nyuu3xubu41zVNeXp4GDhyoc+fOqUuXLmrXrp0uXbqk8PDwAr3m37m4uKhjx47auHGjSpUqJU9PTzVv3ryAWwQAAHBn0S8BsAeEUgAcRtmyZeXj46OkpCTVr19f0pV7KPz444968MEHrZZNTU3Vrl27tG3bNsup5cuXL5dUsF+KMZlM14z94x//UFhYmEqXLq1OnTpddxkAAABbol8CYCRCKQAOZcCAAZo5c6YqVqyoGjVqaN68ecrOzlaXLl2slvPy8pKTk5M++eQTtWvXTnv37lVMTIwkKScn56bv4+HhIUk6cOCAypUrpzJlyigwMFAeHh5at26d3nvvvTu/cQAAAHcA/RIAo/DrewAcysCBA/Xkk0/qtddeU2hoqI4fP653333XcnTvqsqVK2vcuHGaN2+eQkJCFB8fr1dffVUuLi7av3//Td+nfPnyeuyxx/TCCy9YflnGZDKpU6dOqly5sho3blwk2wcAAHC76JcAGMVkLsh5lQCAO2LEiBGqUaOGIiIibF0KAACAXaJfAhwHl+8BgAGSkpL0448/atOmTVq/fr2tywEAALA79EuA4yGUAgADfPvtt1q4cKFefPFF+fn52bocAAAAu0O/BDgeLt8DAAAAAACA4bjROQAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAxHKAUAAAAAAADDEUoBAAAAAADAcIRSAAAAAAAAMByhFAAAAAAAAAz3/yDy5VuejwdrAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.55      0.30      0.39        20\n",
      "    positive       0.92      0.97      0.95       342\n",
      "    negative       0.90      0.82      0.86       111\n",
      "\n",
      "    accuracy                           0.91       473\n",
      "   macro avg       0.79      0.70      0.73       473\n",
      "weighted avg       0.90      0.91      0.90       473\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5273510c98e60e01",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
